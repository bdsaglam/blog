[
  {
    "objectID": "posts/fast-fourier-transform/index.html",
    "href": "posts/fast-fourier-transform/index.html",
    "title": "Fourier Transform",
    "section": "",
    "text": "Fourier transform decomposes a signal into the sinusoidal components with different amplitude, frequency and phases such that the sum of those components equals to the signal. It transforms a signal from time domain to frequency domain. Fourier transform is applicable to both continuous and discrete signals. In this post, we will only cover the discrete case.\nFor a signal with N points, Discrete Fourier Transform uses the following bases:\n\\(e^{i~2\\pi~k} = \\cos (2\\pi~k)+i\\sin (2\\pi~k)\\)\nfor k = -N/2 … N/2 where k=1 means 1 cycle per signal."
  },
  {
    "objectID": "posts/fast-fourier-transform/index.html#nyquists-sampling-theorem",
    "href": "posts/fast-fourier-transform/index.html#nyquists-sampling-theorem",
    "title": "Fourier Transform",
    "section": "Nyquist’s Sampling Theorem",
    "text": "Nyquist’s Sampling Theorem\nTo be able to measure a sinusoidal wave, we need to sample at least two points within its one full cycle, i.e., one point in half cycle. Therefore, we cannot measure the components that make more than N/2 cycles within N timesteps. This is known as Nyquist’s sampling theorem. In practice, because of the risk of sampling those two points near zero crossings, we are only confident about the components with frequencies lower than N/4.\nAs an example, let’s see the measurements from 1 Hz sine wave with various sampling rates. Nyquist’s sampling theorem requires at least 2 Hz sampling rate for this signal. As you can see in the figure below, the sampling rate of 1 Hz (blue) measures a flat line. While sampling rate of 2 Hz (orange) is theoretically sufficent, its amplitude is far from the original’s. As the sampling rate increases, the measurement becomes more accurate.\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\n\n# Original signal\nw = 1 # frequency of sine wave, [Hz]\nT = 3 # duration of signal [second]\nfs = 128 # underlying sampling rate of signal[Hz]\n\nt = np.linspace(0, T, fs * T)\ny = np.sin(2*np.pi*w*t)\nindices = np.arange(len(t))\nfs_list = [1, 2, 4, 10]\n\nsamples = []\nfor f in fs_list:\n    offset = np.random.randint(0, 10)\n    idx = indices[:-offset:fs//f] + offset\n    t_sampled = t[idx]\n    y_sampled = y[idx]\n    samples.append((f, t_sampled, y_sampled))\n\n\n\nCode\nplt.figure(figsize=(12, 8))\nplt.plot(t, y, color='magenta', lw=4);\n\nlegends = ['Original signal']\nfor f, ts, ys in samples:\n    plt.plot(ts, ys, marker='o', ms=10, linestyle='--', lw=4);\n    legends.append(f'Fsamp = {f}ω')\nplt.legend(legends, loc='lower left')\nplt.xlim([0, 2]);"
  },
  {
    "objectID": "posts/fast-fourier-transform/index.html#discrete-fourier-transform",
    "href": "posts/fast-fourier-transform/index.html#discrete-fourier-transform",
    "title": "Fourier Transform",
    "section": "Discrete Fourier Transform",
    "text": "Discrete Fourier Transform\nDiscrete Fourier Transform\n\\[\nX_k = \\sum_{n=0}^{N-1} x_n \\cdot e^{-i~2\\pi~k~n~/~N}\n\\]\nInverse Discrete Fourier Transform\n\\[\nx_n = \\frac{1}{N}\\sum_{k=0}^{N-1} X_k e^{i~2\\pi~k~n~/~N}\n\\]\nThe intuition behind first formula is that kth cosine component takes k/N cycles in 1 timestep or point interval. In other words, its angular velocity is 2πk/N radians/timestep. To make kth component’s peak align with nth point, we need adjust the phase of kth component. Since it takes 2πk/N radians in one timestep, until nth point, it takes 2πkn/N radians. Therefore, we need to delay this component for 2πkn/N radians. To delay, we need to subtract from its phase, which means rotating its complex representation in negative/counter-clock-wise direction. Hence, there is a minus sign in front.\nFrom the above formula, for N point signal, a naive Fourier transform algorithm has O(N2) time complexity. However, Cooley and Tukey (1965) proposed an O(N logN) algorithm; hence, named as Fast Fourier Transform. It benefits the symmetry in the problem and uses recursive divide-conquer approach. Checkout this detailed explanation of FFT algorithm, if you’re interested.\nNumpy provides FFT algorithm in numpy.fft subpackage along with some utilities.\nLet’s have a signal consisting of two sinusoidal waves with 5 Hz and 10 Hz and uniform noise. Since FFT makes recursive calls to divide the signal into two halves, the number of points in the signal must be power of 2.\n\nfs = 128 # sampling rate, [Hz]\nT = 1 # duration of signal, [second]\nt = np.linspace(0, T, fs)\nN = len(t)\n\n\ncomponents = [\n    (1, 5, np.pi/4), # amplitude, frequency [Hz], phase [radian]\n    (2, 10, 0),\n]\n\nsignal = sum([a * np.cos(2*np.pi*w*t + p) for (a, w, p) in components])\nnoise = 1*np.random.uniform(-1, 1, N)\nx = signal + noise\n\n\n\nCode\nplt.figure(figsize=(12, 6))\nplt.plot(t, x)\nplt.xlabel(\"Time [second]\");\n\n\n\n\n\nDFT produces N complex coefficients xk per each component with frequency of k. Here, k denotes normalized frequency with unit of cycle/point and it ranges from -0.5 to 0.5. We can convert these frequencies to Hertz, by substituting point with time interval between consecutive points.\n\\[\n\\frac{cycle}{point} = \\frac{cycle}{\\frac{t~seconds}{N~points}} = \\frac{N}{t} * \\frac{cycle}{second} = \\frac{N}{t}~Hz\n\\]\nNumpy provides a utility function numpy.fft.fftfreq that takes number of points in DFT and timestep which is multiplicative inverse of sampling rate.\n\nN = len(signal)\nxk = np.fft.fft(x)\nfreq = np.fft.fftfreq(n=N, d=1/fs)\n\n\nprint(xk[:5])\n\n[12.07258851 +0.j          3.04180456+13.42630117j\n  2.4691799  -4.18227651j  1.64226413 -0.16802957j\n  5.91780044 +1.05461309j]\n\n\n\nprint(freq)\n\n[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n  28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n  42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n  56.  57.  58.  59.  60.  61.  62.  63. -64. -63. -62. -61. -60. -59.\n -58. -57. -56. -55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45.\n -44. -43. -42. -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31.\n -30. -29. -28. -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17.\n -16. -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.\n  -2.  -1.]\n\n\n\n# the frequencies\nnormalized_freq = np.fft.fftfreq(n=N)\nfreq_manual = normalized_freq * N / T\n\nassert np.allclose(freq, freq_manual)\n\n\n# Bring negative frequencies to the front of the array\nfr = np.roll(freq, N//2)\namp = np.roll(np.abs(xk), N//2)\nphase = np.roll(np.angle(xk), N//2)\n\n\n\nCode\nplt.figure(figsize=(12, 4))\nplt.bar(fr, amp)\nplt.ylabel(\"Amplitude\");\nplt.xlabel(\"Frequency [Hz]\")\n\n\nText(0.5, 0, 'Frequency [Hz]')"
  },
  {
    "objectID": "posts/fast-fourier-transform/index.html#inverse-discrete-fourier-transform",
    "href": "posts/fast-fourier-transform/index.html#inverse-discrete-fourier-transform",
    "title": "Fourier Transform",
    "section": "Inverse Discrete Fourier Transform",
    "text": "Inverse Discrete Fourier Transform\n\\[\nx_n = \\frac{1}{N}\\sum_{k=0}^{N-1} X_k e^{i~2\\pi~k~n~/~N}\n\\]\nNow, we have the information about each of the components in the signal. We can reconstruct the original signal, by combining those components. For this, we first convert each coefficient xk into cosine and sine waves by multiplying with \\(e^{i~2\\pi~k~n~/~N}\\). Then, we sum all these waves per component and normalize it by number of points.\nNumpy provides inverse FFT function numpy.fft.ifft, which takes N complex coefficients xk and outputs N complex numbers with very small imaginary parts. Remember,\n\\[e^{ix} = \\cos (x)+i\\sin (x)\\] \\[ cos(-x) = cos(x) \\] \\[ sin(-x) = -sin(x) \\]\nTherefore, while summing up the components, the imaginary parts (sine) for negative and positive frequencies cancels each other. Whereas, real parts (cosine) add up.\n\nrec_signal = np.real(np.fft.ifft(xk))\n\n\n\nCode\nplt.figure(figsize=(12, 6))\nplt.plot(signal, lw=2)\nplt.plot(rec_signal, lw=2)\nplt.xlabel(\"Time [second]\")\nplt.legend([\"Original\", \"Reconstructed\"]);"
  },
  {
    "objectID": "posts/fast-fourier-transform/index.html#components",
    "href": "posts/fast-fourier-transform/index.html#components",
    "title": "Fourier Transform",
    "section": "Components",
    "text": "Components\nLet’s see how FFT predicts the amplitude, frequency and phase parameters of the components.\n\n\nCode\ndef get_component_params(xk, freq):\n    component = 2 * xk[freq] # multiply by two to include -freq as well\n    phase = np.angle(component)\n    amplitude = np.abs(component) / N\n    return amplitude, phase\n\n\n\n\nCode\ndef make_component_signal(xk, freq, t):\n    amplitude, phase = get_component_params(xk, freq)\n    component_signal = amplitude * np.cos(2 * np.pi * freq * t + phase)\n    return component_signal\n\n\n\n\nCode\nfor a, f, ph in components:\n    amp, phase = get_component_params(xk, f)\n    print()\n    print(f\"Component {f} Hz\")\n    print('_'*40)\n    print()\n    print(f\"True  amplitude: {a:4.2f} phase: {np.rad2deg(ph):4.1f} deg\")\n    print(f\"Calc. amplitude: {amp:4.2f} phase: {np.rad2deg(phase):4.1f} deg\")\n    print('='*40)\n\n\n\nComponent 5 Hz\n________________________________________\n\nTrue  amplitude: 1.00 phase: 45.0 deg\nCalc. amplitude: 1.07 phase: 52.1 deg\n========================================\n\nComponent 10 Hz\n________________________________________\n\nTrue  amplitude: 2.00 phase:  0.0 deg\nCalc. amplitude: 1.97 phase: 17.2 deg\n========================================\n\n\nAs we can see, it is fairly accurate considering the noise added on the signal.\n\n\nCode\nc5 = make_component_signal(xk, 5, t)\nc10 = make_component_signal(xk, 10, t)\n\nplt.figure(figsize=(12, 6))\nplt.plot(t, signal, linewidth=2)\nplt.plot(t, c5, t, c10)\nplt.xlabel(\"Time [second]\")\nplt.legend(['Original signal', '5 Hz component', '10 Hz component']);"
  },
  {
    "objectID": "posts/fast-fourier-transform/index.html#conclusion",
    "href": "posts/fast-fourier-transform/index.html#conclusion",
    "title": "Fourier Transform",
    "section": "Conclusion",
    "text": "Conclusion\nFFT transforms a signal from time domain to frequency domain and for some problems, frequency domain is more feasible to work with. FFT has usages in many fields such as * Solving differential equations * Signal filtering algorithms * System identification"
  },
  {
    "objectID": "posts/fastai-custom-model/index.html",
    "href": "posts/fastai-custom-model/index.html",
    "title": "How to use a custom model with fastai cnn_learner?",
    "section": "",
    "text": "Code\ndef pprint_model(model, truncate=64):\n    print(\"=\"*80)\n    print(\"Model modules:\")\n    print(\"=\"*80)\n    print()\n    for i, (name, module) in enumerate(model.named_children()):\n        desc = repr(module).replace('\\n', '').replace('  ', ' ')[:64] + '...'\n        print(f\"{i+1} - {desc}\\n\")\nfastai library offers many pre-trained models for vision tasks. However, we sometimes need to use a custom model available in another library or created from scratch. In this post, we’ll see how to use fastai’s cnn_learner with a custom model.\ncnn_learner is a utility function which creates a Learner from given a pretrained CNN architecture such as resnet18.\nTo do that, it uses the model metadata from model_meta registry. model_meta registry is simply a mapping (dictionary) from architecture to its metadata.\nThe cut value is used for stripping off the existing classification head of the network so that we can add a custom head and fine-tune it for our task.\nThe split function is used when discriminative learning rate schema is applied such that the layers of a model are trained with different learning rates.\nThe stats refer to the channel means and standard deviations of the images in ImageNet dataset, which the model is pretrained on.\nThere are two alternative ways to to use a custom model not present in model registry: 1. Create a new helper function similar to cnn_learner that splits the network into backbone and head. Check out Zachary Mueller’s awesome blog post to see how it’s done. 2. Register the architecture in model_meta and use cnn_learner.\nWe will cover the second option in this post.\nLet’s first inspect an architecture registered already, e.g. resnet18.\nHere is its model meta data from the registry:\nfrom fastai.vision.all import *\nmodel_meta[resnet18]\n\n{'cut': -2,\n 'split': &lt;function fastai.vision.learner._resnet_split&gt;,\n 'stats': ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])}\nAnd the model layers:\nm = resnet18()\npprint_model(m)\n\n================================================================================\nModel modules:\n================================================================================\n\n1 - Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3),...\n\n2 - BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_runn...\n\n3 - ReLU(inplace=True)...\n\n4 - MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_m...\n\n5 - Sequential( (0): BasicBlock(  (conv1): Conv2d(64, 64, kernel_siz...\n\n6 - Sequential( (0): BasicBlock(  (conv1): Conv2d(64, 128, kernel_si...\n\n7 - Sequential( (0): BasicBlock(  (conv1): Conv2d(128, 256, kernel_s...\n\n8 - Sequential( (0): BasicBlock(  (conv1): Conv2d(256, 512, kernel_s...\n\n9 - AdaptiveAvgPool2d(output_size=(1, 1))...\n\n10 - Linear(in_features=512, out_features=1000, bias=True)...\ncreate_body function called by create_cnn_model which is called in cnn_learner, strips off the head by cut index as such:\nIn our case, it’ll remove the last two layers of resnet18 network: AdaptiveAvgPool2d and fully connected Linear layer.\nbody = create_body(resnet18, pretrained=False, cut=model_meta[resnet18]['cut'])\npprint_model(body)\n\n================================================================================\nModel modules:\n================================================================================\n\n1 - Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3),...\n\n2 - BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_runn...\n\n3 - ReLU(inplace=True)...\n\n4 - MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_m...\n\n5 - Sequential( (0): BasicBlock(  (conv1): Conv2d(64, 64, kernel_siz...\n\n6 - Sequential( (0): BasicBlock(  (conv1): Conv2d(64, 128, kernel_si...\n\n7 - Sequential( (0): BasicBlock(  (conv1): Conv2d(128, 256, kernel_s...\n\n8 - Sequential( (0): BasicBlock(  (conv1): Conv2d(256, 512, kernel_s...\nSimilarly, we need to determine the cut index for the custom model we use. Let’s try EfficientNetB0 architecture available in torchvision library. First, we inspect the network layers to find out where to split it into backbone and head.\nfrom torchvision.models import efficientnet_b0\n\nm = efficientnet_b0()\npprint_model(m)\n\n================================================================================\nModel modules:\n================================================================================\n\n1 - Sequential( (0): ConvNormActivation(  (0): Conv2d(3, 32, kernel_...\n\n2 - AdaptiveAvgPool2d(output_size=1)...\n\n3 - Sequential( (0): Dropout(p=0.2, inplace=True) (1): Linear(in_fea...\nAs it can be seen, the pooling layer is at index -2, which corresponds to the cut value. We’ll use the default_split for split function and ImageNet stats as the model is pre-trained on it.\nfrom fastai.vision.learner import default_split\nmodel_meta[efficientnet_b0] = {'cut': -2, 'split': default_split, 'stats': imagenet_stats}"
  },
  {
    "objectID": "posts/fastai-custom-model/index.html#train-and-test-the-model",
    "href": "posts/fastai-custom-model/index.html#train-and-test-the-model",
    "title": "How to use a custom model with fastai cnn_learner?",
    "section": "Train and test the model",
    "text": "Train and test the model\nNow we can create a cnn_learner since our custom architecture is registered. Let’s create a toy dataloaders to train and test our model.\n\ndef label_func(f): \n    return f[0].isupper()\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path / \"images\")\npattern = r'^(.*)_\\d+.jpg'\ndls = ImageDataLoaders.from_name_re(path, files, pattern, item_tfms=Resize(224))\n\ndls.show_batch()\n\n\n    \n        \n      \n      100.00% [811712512/811706944 00:20&lt;00:00]\n    \n    \n\n\n\n\n\n\nlearn = cnn_learner(\n    dls, \n    arch=efficientnet_b0,\n    pretrained=True,\n    metrics=accuracy, \n).to_fp16()\n\nDownloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-3dd342df.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-3dd342df.pth\n\n\n\n\n\nLet’s verify that the body and head are created correctly.\n\npprint_model(learn.model)\n\n================================================================================\nModel modules:\n================================================================================\n\n1 - Sequential( (0): Sequential(  (0): ConvNormActivation(   (0): Co...\n\n2 - Sequential( (0): AdaptiveConcatPool2d(  (ap): AdaptiveAvgPool2d(...\n\n\n\nLet’s inspect the custom head of the model:\n\npprint_model(learn.model[-1])\n\n================================================================================\nModel modules:\n================================================================================\n\n1 - AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp...\n\n2 - Flatten(full=False)...\n\n3 - BatchNorm1d(2560, eps=1e-05, momentum=0.1, affine=True, track_ru...\n\n4 - Dropout(p=0.25, inplace=False)...\n\n5 - Linear(in_features=2560, out_features=512, bias=False)...\n\n6 - ReLU(inplace=True)...\n\n7 - BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_run...\n\n8 - Dropout(p=0.5, inplace=False)...\n\n9 - Linear(in_features=512, out_features=37, bias=False)...\n\n\n\nAs it’s seen, cnn_learner created a new classification head starting with a pooling layer while keeping the original body from the pre-trained model.\nLet’s train and test our model for multi-label classification task.\n\nlearn.lr_find(start_lr=1e-05, end_lr=1e-1)\n\n\n\n\nSuggestedLRs(valley=0.0014454397605732083)\n\n\n\n\n\n\nlearn.fine_tune(3, base_lr=2e-3, freeze_epochs=3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.515583\n0.687396\n0.801759\n01:12\n\n\n1\n1.083641\n0.469568\n0.852503\n01:08\n\n\n2\n0.687156\n0.430938\n0.866035\n01:07\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.414339\n0.401623\n0.876861\n01:27\n\n\n1\n0.351286\n0.369699\n0.872801\n01:27\n\n\n2\n0.272294\n0.349316\n0.881597\n01:27\n\n\n\n\n\n\nlearn.show_results()"
  },
  {
    "objectID": "posts/tgi-litellm-openai/index.html",
    "href": "posts/tgi-litellm-openai/index.html",
    "title": "Serving open source LLM as OpenAI API",
    "section": "",
    "text": "In this tutorial, we’ll see how to serve an open source language model with OpenAI compatible API using HuggingFace Text Generation Inference and LiteLLM OpenAI proxy server. This enables us to use any tool compatible with OpenAI API.\nWe’ll use 4-bit quantized Llama-2 7B chat model to lower GPU memory requirement. Launch TGI server on a device with GPU:\n#!/bin/bash\n\nVOLUME=\"${HOME}/.cache/huggingface/tgi\"\nmkdir -p $VOLUME\n\ndocker run --gpus all --shm-size 1g \\\n    -p 8080:80 \\\n    -v \"${VOLUME}\":/data \\\n    ghcr.io/huggingface/text-generation-inference:latest \\\n     --trust-remote-code \\\n     --model-id \"NousResearch/llama-2-7b-chat-hf\" \\\n     --quantize bitsandbytes-nf4 \\\n     --dtype float16\nHuggingFace Text Generation Inference server supports only text completion. However, we want to send chat messages with system, user, and assistant roles same as OpenAI models (i.e. ChatML format). Fortunately, LiteLLM supports Llama-2 chat template, which will convert chat messages to text prompt before calling TGI server. All we need to do is to specify model paramater as huggingface/meta-llama/Llama-2-7b. For models not supported by LiteLLM, we can create a custom template.\nHere is the config for LiteLLM OpenAI proxy Server.\n# config.yaml\nmodel_list:\n  - model_name: llama-2-7b-chat # arbitrary alias for our mdoel\n    litellm_params: # actual params for litellm.completion()\n      model: \"huggingface/meta-llama/Llama-2-7b\"\n      api_base: \"http://localhost:8080/\"\n      max_tokens: 1024\n\nlitellm_settings:\n  set_verbose: True\nLaunch LiteLLM OpenAI proxy server:\nlitellm --config config.yaml\nSet API base url.\n\nAPI_BASE=\"http://localhost:8000/\"\n\nLet’s use the model with completion function provided by litellm library, first.\n\nfrom litellm import completion \n\nmessages = [\n    {\"content\": \"You are helpful assistant.\",\"role\": \"system\"},\n    {\"content\": \"Tell me 3 reasons to live in Istanbul.\",\"role\": \"user\"},\n]\n\nresponse = completion(\n  api_base=API_BASE,\n  model=\"llama-2-7b-chat\", \n  custom_llm_provider=\"openai\", # so that messages are sent to proxy server as they are\n  messages=messages, \n  temperature=0.3,\n)\n\nprint(response.choices[0].message.content)\n\n\nIstanbul is a city with a rich history and culture, and there are many reasons to live there. Here are three:\n\n1. Cultural Diversity: Istanbul is a city that straddles two continents, Europe and Asia, and has a unique cultural identity that reflects its history as a crossroads of civilizations. The city is home to a diverse population, including Turks, Kurds, Greeks, Armenians, and other ethnic groups, each with their own traditions and customs. This diversity is reflected in the city's architecture, food, music, and art, making Istanbul a vibrant and exciting place to live.\n2. Historical Landmarks: Istanbul is home to some of the most impressive historical landmarks in the world, including the Hagia Sophia, the Blue Mosque, and the Topkapi Palace. These landmarks are not only important cultural and religious sites, but also serve as a reminder of the city's rich history and its role in the development of civilizations. Living in Istanbul, you are surrounded by these incredible structures, which are a source of inspiration and pride for the city's residents.\n3. Gastronomy: Istanbul is known for its delicious and diverse food scene, which reflects the city's cultural diversity. From traditional Turkish dishes like kebabs and baklava, to Greek and Middle Eastern cuisine, there is something for every taste and budget. Living in Istanbul, you have access to a wide range of fresh produce, spices, and other ingredients, which are used to create mouth-watering dishes that are both healthy and delicious.\n\nOverall, Istanbul is a city that offers a unique and enriching experience for those who live there. Its cultural diversity, historical landmarks, and gastronomy make it a vibrant and exciting place to call home.\n\n\nNow, let’s use the model with llama-index library. The subtle point is that LiteLLM class in llama-index expects custom_llm_provider parameter in additional_kwargs argument.\n\nfrom llama_index.llms import ChatMessage, LiteLLM\n\nllm = LiteLLM(\n    api_base=API_BASE,\n    api_key=\"\",\n    model=\"llama-2-7b-chat\", \n    temperature=0.3,\n    additional_kwargs=dict(\n        custom_llm_provider=\"openai\", # so that messages are sent to proxy server as they are\n    ),\n)\n\nmessages = [\n    ChatMessage.parse_obj({\"content\": \"You are helpful assistant.\", \"role\": \"system\"}),\n    ChatMessage.parse_obj({\"content\": \"Tell me 3 reasons to live in London.\", \"role\": \"user\"}),\n]\nresponse = llm.chat(messages) \nprint(response.message.content)\n\n\n\n1. Cultural diversity: London is a melting pot of cultures, with people from all over the world calling it home. This diversity is reflected in the city's food, art, music, and fashion, making it a vibrant and exciting place to live.\n2. World-class amenities: London has some of the best amenities in the world, including top-notch restaurants, theaters, museums, and sports venues. Whether you're looking for a night out on the town or a quiet evening at home, London has something for everyone.\n3. Investment opportunities: London is a major financial hub, with many opportunities for investment in real estate, business, and other industries. The city's strong economy and stable political environment make it an attractive place to invest and grow your wealth.\n\n\n\nmessages = [\n    ChatMessage(content=\"You are an hilarious comedian who is famous with their sarcastic jokes.\", role=\"system\"),\n    ChatMessage(content=\"Tell me a joke about front-end developers.\", role=\"user\"),\n]\nresponse = llm.chat(messages) \nprint(response.message.content)\n\n\n\nI'm glad you think I'm hilarious! Here's a joke for you:\n\nWhy did the front-end developer break up with his girlfriend?\n\nBecause he wanted a more responsive relationship! 😂\n\nI hope you found that one amusing! Front-end developers can be a funny topic, but I'm sure they won't mind a good-natured jab or two. Let me know if you want another one!"
  },
  {
    "objectID": "posts/confidence-intervals/index.html",
    "href": "posts/confidence-intervals/index.html",
    "title": "How many coin flips does high confidence need?",
    "section": "",
    "text": "Code\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport seaborn\n\n# Set seaborn aesthetic parameters to defaults\nseaborn.set()\nseaborn.set_style('whitegrid')\nseaborn.set_palette(\"dark\")\n\nimport math\nimport numpy as np\nfrom scipy.special import comb\n\nnp.random.seed(42)\nYou walked into a casino, there is a game of tossing coins. You don’t know whether the coin is biased or fair. But, you can observe the game and keep the stats. How many tosses would you observe to measure the probability of the coin?\nThough it is not possible to exactly determine the underlying probability distribution of the coin, we can measure in high confidence, if we observe enough tosses.\nThe probability distribution of a coin toss is known as Bernoulli distribution parameterized with p, the probability of heads. There is no way we can directly measure this. Instead, we count the heads, \\(k\\), and divide it into total number of tosses, \\(n\\). How do we know that this is a good way of measuring p? Well, the probability distribution of number of heads is Binomial distribution, parameterized with p and n. And the expectation of Bernoulli distribution is \\(np\\). Therefore, if we count heads and divide it by total number of tosses, we have an estimate of \\(p\\).\n\\[\n\\text{Binomial Distribution } \\\\\n\\]\n\\[\nP_{p,n}(k: \\text{\\#heads}) =  \\binom{N}{k} \\cdot p^k \\cdot (1-p)^{n-k}\n\\]\n\\[\n\\mathbb{E}[\\#heads]= \\sum\\limits_{i=1}^n  \\mathbb{E}[\\mathbb{1}[coin_i]] =\n\\sum\\limits_{i=1}^n  p = np  \\tag*{(Linearity of Expectation) }\n\\]\n\\[\n\\mathbb{1}[coin_i] = 1   \\hspace{35pt}\\text{if ith coin comes heads}\\\\\n0 \\hspace{48pt} \\text{otherwise} \\\\\n\\]\nLet’s simulate this for 100 tosses. First, let’s randomly pick \\(p\\) and see what our estimation ends up.\nn = 100\np = np.random.rand(1)[0]\ntosses = np.random.rand(n)\nheads = tosses &lt; p\n\nspace = np.array(['Heads', 'Tails'])\nprint(space[heads.astype(np.int)])\nprint()\n\np_est = heads.sum() / n\nerror = abs(p - p_est)\nprint(f\"True p\\t\\t: {p}\")\nprint(f\"Estimated p\\t: {p_est}\")\nprint(f\"Error\\t\\t: {error:%}\")\n\n['Heads' 'Heads' 'Heads' 'Tails' 'Tails' 'Tails' 'Heads' 'Heads' 'Heads'\n 'Tails' 'Heads' 'Heads' 'Tails' 'Tails' 'Tails' 'Tails' 'Heads' 'Heads'\n 'Tails' 'Heads' 'Tails' 'Tails' 'Tails' 'Heads' 'Heads' 'Tails' 'Heads'\n 'Heads' 'Tails' 'Heads' 'Tails' 'Tails' 'Heads' 'Heads' 'Heads' 'Tails'\n 'Tails' 'Heads' 'Heads' 'Tails' 'Heads' 'Tails' 'Heads' 'Tails' 'Heads'\n 'Tails' 'Heads' 'Heads' 'Tails' 'Heads' 'Heads' 'Heads' 'Heads' 'Heads'\n 'Heads' 'Tails' 'Tails' 'Tails' 'Tails' 'Heads' 'Tails' 'Heads' 'Tails'\n 'Tails' 'Heads' 'Tails' 'Heads' 'Tails' 'Heads' 'Heads' 'Tails' 'Tails'\n 'Heads' 'Heads' 'Heads' 'Heads' 'Tails' 'Tails' 'Tails' 'Heads' 'Heads'\n 'Tails' 'Tails' 'Tails' 'Tails' 'Heads' 'Heads' 'Heads' 'Heads' 'Tails'\n 'Heads' 'Heads' 'Heads' 'Heads' 'Heads' 'Heads' 'Heads' 'Tails' 'Tails'\n 'Tails']\n\nTrue p      : 0.3745401188473625\nEstimated p : 0.45\nError       : 7.545988%\n7% error! Way too much.\nLet’s repeat this experiment for different sample sizes and see how the error changes.\ndef measure(p, n):\n    tosses = np.random.rand(n)\n    heads = tosses &lt; p\n    p_est = heads.sum() / n\n    return p_est\nsizes = np.array([2**i for i in range(1, 15)])\nestimates = np.array([measure(p, int(size)) for size in sizes])\nerrors = np.abs(estimates - p)\nplt.figure()\nplt.scatter(sizes, errors)\nplt.xscale('log')\nThough there are some deviations, it seem’s like error decreases as the sample size grows. But, how to know where to stop? Also, how confident are we about our measurements considering the deviations in them? Fortunately, with the help of probability theory, we can determine the number of samples required to reach the desired confidence level for our estimate."
  },
  {
    "objectID": "posts/confidence-intervals/index.html#confidence-intervals",
    "href": "posts/confidence-intervals/index.html#confidence-intervals",
    "title": "How many coin flips does high confidence need?",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nWe want to ensure that our estimate is close to true value of \\(p\\). In other words, we want the probability of the event that our estimate is within a small interval around \\(p\\) as high as possible. This probability, a.k.a. confidence level, can be formulated as such,\n\\[\n\\text{confidence level} = P[|p_{est} - p| \\leq \\text{interval}] \\\\\n= 1 - P[|p_{est} - p| &gt; \\text{interval}]\n= 1 - P[p_{est} &gt; p + \\text{interval} \\hspace{4pt} \\cup \\hspace{4pt} p_{est} &lt; p - \\text{interval}] \\\\\n= 1 - P[\\frac{k}{n} &gt; p + \\text{interval} \\hspace{4pt} \\cup \\hspace{4pt} \\frac{k}{n} &lt; p - \\text{interval}] \\\\\n= 1 - (P[k &gt; n (p + \\text{interval})] + P[k &lt; n (p - \\text{interval})])\n\\]\n\\[\nP[k &gt; n (p + \\text{interval})] = P[n-k &lt; n - n (p + \\text{interval})] = P[n-k &lt; n (1 - p - \\text{interval}))]\n\\]\n\\[\n\\text{confidence level} = 1 - (P[n-k &lt; n (1 - p - \\text{interval}))] + P[k &lt; n (p - \\text{interval})]) \\\\\n= 1 - ( F_{n,1-p}[n(1 - p - \\text{interval})] + F_{n,p}[n(p - \\text{interval})] ) \\\\\nF: \\text{Cumulative Distribution Function}\n\\]\nSo, we need to calculate cumulative mass function (CMF) of Binomial distribution parameterized with \\(p\\) and \\(n\\). But, we don’t know p. It turns out that the above expression takes it minimum value at \\(p=0.5\\). Therefore, we can find a lower bound on confidence level with \\(p=0.5\\). The formula for confidence level becomes,\n\\[\n\\text{confidence level} &gt;= 1 - ( F_{n,0.5}[n(0.5 - \\text{interval})] + F_{n,0.5}[n(0.5 - \\text{interval})] ) \\\\\n= 1 - 2 F_{n,0.5}[n(0.5 - \\text{interval})]\n\\]\nSince we know the probability mass function (PMF) of Binomial Distribution, it is easy to calculate CMF.\n\\[\nF_x[X] = P_x[x &lt; X] = \\sum\\limits_{i=0}^{X-1} P[x=i]\n\\]\nLet’s define PMF by using scipy.special.comb for combination function.\n\nfrom scipy.special import comb\n\n\ndef binomial_pmf_naive(n, p, k):\n    return comb(n, k) * p ** k * (1-p)**(n-k)\n\n\nbinomial_pmf_naive(2, 0.5, 0)\n\n0.25\n\n\n\nbinomial_pmf_naive(100, .5, 50)\n\n0.07958923738717873\n\n\n\nbinomial_pmf_naive(2000, .5, 1000)\n\ninf\n\n\nAs you see, for big n values, Scipy’s comb method overflows since it is out of bounds of Python’s floating point number. To overcome this, we can use an approximation for log of combination function.\n\\[\n\\binom{N}{k} \\sim \\frac{2^{nH(k/n)}}{\\sqrt{2\\pi k (1 - \\frac{k}{n})}}\n\\] \\[\nH(a) = a \\log \\frac{1}{a} + (1-a) \\log \\frac{1}{1-a} \\tag*{[Entropy function]}\n\\]\n\ndef entropy_function(a, eps=1e-32):\n    return a * np.log2(1 / (a + eps)) + (1 - a) * np.log2(1 / (1 - a + eps))\n\n\n\nCode\nx = np.linspace(0, 1, 100)\ny = entropy_function(x) \n\nfig, ax = plt.subplots()\nax.plot(x, y)\nax.set_title(\"Entropy function\");\n\n\n\n\n\n\n@np.vectorize\ndef log_comb_approximate(n, k, eps=1e-32):\n    if k==0 or k==n: return float(\"-inf\")\n    if k==1 or k==(n-1): return np.log(n)\n    \n    a = k/n\n    return n * entropy_function(a) * np.log(2) - np.log(np.sqrt(2 * np.pi * a * (1-a) * n) + eps)\n\n\n\nCode\nn = 1000\nplt.figure(figsize=(8, 4))\nplt.plot(comb(n, np.arange(n+1)), color='blue')\nplt.plot(np.exp(log_comb_approximate(n, np.arange(n+1))).T, color='red')\nplt.legend(['scipy comb', 'approx. comb']);\n\n\n\n\n\n\n\nCode\nn = 100\nk = np.arange(n)\nexpected = comb(n, k)\napproximate = np.exp(log_comb_approximate(n, k))\n\neps = 1e-8\nrel_error = np.abs(approximate/(expected+eps) - 1)\n\nfig, ax = plt.subplots(figsize=(8, 4))\nax.semilogy(k, rel_error)\nax.set_title('Combination function approximation error');\n\n\n\n\n\nOK, the approximation seems pretty close. Now, we can define PMF.\n\ndef binomial_pmf(n, p, k, eps=1e-32):\n    if isinstance(p, (float, int)):\n        p = [p]\n    if isinstance(k, (float, int)):\n        k = [k]\n    p = np.array(p)[:, None]\n    k = np.array(k)[None, :]\n    return np.exp(log_comb_approximate(n, k) + k * np.log(p + eps) + (n - k) * np.log(1 - p + eps))\n\n\n\nCode\nn = 2000\np = 0.4\nk = np.arange(n+1)\npmf = binomial_pmf(n, p, k)\n\nplt.figure(figsize=(12, 6))\nplt.plot(k, pmf.T)\nplt.xlabel(\"Number of heads\")\nplt.ylabel(\"Probability Mass Function\")\nplt.title(f\"Binomial Distribution with p={p}\");\n\n\n\n\n\nLet’s sanity-check approximated PMF.\n\nprint(pmf.sum())\n\n1.0001320546143024\n\n\nCMF is just cumulative sum of PMF.\n\n\nCode\ncmf = pmf.cumsum(-1)\n\nplt.figure(figsize=(12, 6))\nplt.plot(k, cmf.T)\nplt.xlabel(\"Number of heads\")\nplt.ylabel(\"Cumulative Mass Function\")\nplt.title(f\"Binomial Distribution with p={p}\");\n\n\n\n\n\nNow, let’s define a function that calculates confidence level for given sample size and interval.\n\\[\\text{confidence level} \\geq 1 - 2 F_{n,0.5}[n(0.5 - \\text{interval})]\\]\n\ndef compute_confidence(n, interval):\n    p = 1/2\n    low = np.ceil(n*(p-interval)).astype(np.int)\n    pmf = binomial_pmf(n, p, k=np.arange(low+1)).squeeze()\n    cmf = pmf.cumsum()\n    return 1 - 2 * cmf[low]\n\nLet’s say we want a tight interval around true value of \\(p\\), so we set it 0.01.\n\ninterval = 0.01\n\nAnd for sample size of \\(n=100\\), the minimum confidence becomes,\n\nn = 100\nconf = compute_confidence(n, interval)\nprint(f\"confidence level &gt;{conf: %}\")\n\nconfidence level &gt; 7.725091%\n\n\nThis is very low, we definitely need more samples. Let’s go for \\(n=1000\\).\n\nn=1000\nconf = compute_confidence(n, interval)\nprint(f\"confidence level &gt;{conf: %}\")\n\nconfidence level &gt; 45.188993%\n\n\nIt gets better, but we cannot keep trying all values one by one. Let’s plot confidence level against different sample sizes.\n\n\nCode\nimport matplotlib.ticker as mtick\n\nns = np.power(2, np.arange(6, 18)) # sample sizes\nconfs = np.array([compute_confidence(n, interval) for n in ns]).squeeze()\n\nplt.figure(figsize=(12, 8))\nplt.semilogx(ns, confs*100)\nplt.ylabel(\"Confidence Level\")\nplt.xlabel('Sample size')\nplt.title(f\"interval={interval}\")\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n\n\n\n\n\nWe need more than 10,000 samples to have confident estimate of \\(p\\).\nLet’s see how it changes with different interval values.\n\n\nCode\nintervals = np.arange(1, 5) / 100\nns = np.power(2, np.arange(6, 17)) # sample sizes\nconfs = np.array([[compute_confidence(n, interval) for n in ns] for interval in intervals]).squeeze()\n\nplt.figure(figsize=(12, 8))\nplt.semilogx(ns, confs.T*100, lw=3)\nplt.ylabel(\"Confidence Level\")\nplt.xlabel('Sample size')\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\nplt.legend([f'{interval}' for interval in intervals]);\n\n\n\n\n\nAs interval gets tighter, we need more samples for a confidence level. Now, let’s write a function that calculates required sample size for given interval and confidence level.\n\n\nCode\nplt.figure(figsize=(12, 8))\nplt.semilogy(ns, (1 - confs.T)*100, lw=3)\nplt.ylabel(\"Error Level\")\nplt.xlabel('Sample size')\nplt.legend([f'{interval}' for interval in intervals]);\n\n\n\n\n\nThe error (\\(1 - \\text{confidence level}\\)) gets exponentially smaller as the sample size increases. Hence, we fit a 1D spline linear interpolator on logarithm of error with respect to sample size.\n\nfrom scipy.interpolate import interp1d\n\ndef find_sample_size(confidence, interval, p=1/2, eps=1e-32):\n    assert interval &gt;= .01\n    \n    ns = np.power(2, np.arange(1, 18))\n    confs = np.array([compute_confidence(n, interval) for n in ns]).squeeze()\n    confs = np.clip(confs, 0, 1)\n    inconfs = 1 - confs\n    f = interp1d(np.log(inconfs + eps), ns, kind='slinear')\n    return f(np.log(1-confidence))\n\nLet’s check for various confidence levels.\n\n\nMin sample size for conf=0.95, interval=0.04 is 659\n\n\n\n\nMin sample size for conf=0.95, interval=0.01 is 9943\n\n\n\n\nMin sample size for conf=0.999999, interval=0.01 is 60058"
  },
  {
    "objectID": "posts/confidence-intervals/index.html#conclusion",
    "href": "posts/confidence-intervals/index.html#conclusion",
    "title": "How many coin flips does high confidence need?",
    "section": "Conclusion",
    "text": "Conclusion\nLet’s say we tossed the coin for 659 times and the number of heads is 309. So, our estimate for \\(p\\) is 0.47. Does this mean \\(p\\) is likely to be in 0.47 ± 0.04? No, quite the opposite. The probability of heads is not probabilistic, it is a characteristic of the coin. We are trying to measure it. On the contrary, our estimate is probabilistic as we take finite number of samples to derive it. Hence, we can only talk about the likelihood of the estimate being close to true value. Specifically, the confidence interval states that there is 95% chance that the estimate is within ±4% interval of true value.\nThis method can be used to determine number of samples required for polls, if we assume that each individual’s answers are mutually independent. The good news is the sample size required does not depend on population size and confidence level gets exponentially better w.r.t. sample size."
  },
  {
    "objectID": "posts/confidence-intervals/index.html#references",
    "href": "posts/confidence-intervals/index.html#references",
    "title": "How many coin flips does high confidence need?",
    "section": "References",
    "text": "References\n\nEric Lehman, F. Thomson Leighton, Albert R. Meyer. Mathematics for Computer Science. Lecture Notes, available at https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-fall-2010/readings/MIT6_042JF10_notes.pdf, 2010."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Beneath the code and cosmic splendor,\nA software engineer, a dreamer, a ponderer,\nReaching for AI, strong as thunder,\nIn this vast universe, a knowledge wanderer.\nSeeking super-human minds, intelligence to create,\nDenizens of cosmos, our limits we break,\nEntering the unknown, our legacy we make,\nNavigating the stars, our destiny we take.\nIn pursuit of immortality, as fates intertwine,\nZealous advances, medical marvels that shine,\nSolving mysteries with math, a race through time,\nAdept in physics, unraveling the cosmic design.\nGraceful on the tennis court, I embrace,\nLatin dances fill my heart, a rhythmic trace,\nA whirlwind of movement, through time and space,\nMoments in nature, where sunlight I chase.\nLending my thoughts to a digital space,\nOn the blog, I share wisdom with grace,\nVenturing into the realms of machine learning,\nEmbracing the secrets of AI, passion burning.\nScribing tales of software engineering,\nA worldwide community, knowledge we're bringing,\nIn the pursuit of the stars, our legacy endures."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Curiosity Trace",
    "section": "",
    "text": "Serving open source LLM as OpenAI API\n\n\n\n\n\n\n\nllm\n\n\nhuggingface\n\n\nopenai\n\n\nlitellm\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2024\n\n\n\n\n\n\n  \n\n\n\n\nHow to use HuggingFace datasets with fastai?\n\n\n\n\n\n\n\nfastai\n\n\nhuggingface\n\n\ndatasets\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2023\n\n\n\n\n\n\n  \n\n\n\n\nHow to use a custom model with fastai cnn_learner?\n\n\n\n\n\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2022\n\n\n\n\n\n\n  \n\n\n\n\nHow many coin flips does high confidence need?\n\n\n\n\n\n\n\nmath\n\n\n\n\n\n\n\n\n\n\n\nJul 25, 2020\n\n\n\n\n\n\n  \n\n\n\n\nWhy is initialization important in neural networks?\n\n\n\n\n\n\n\nneural-network\n\n\n\n\n\n\n\n\n\n\n\nJun 25, 2020\n\n\n\n\n\n\n  \n\n\n\n\nFourier Transform\n\n\n\n\n\n\n\nmath\n\n\n\n\nA minimal introduction\n\n\n\n\n\n\nJun 23, 2020\n\n\n\n\n\n\n  \n\n\n\n\nHalting problem\n\n\n\n\n\n\n\ncomputer-science\n\n\n\n\n\n\n\n\n\n\n\nApr 7, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/initialization-in-nn/index.html",
    "href": "posts/initialization-in-nn/index.html",
    "title": "Why is initialization important in neural networks?",
    "section": "",
    "text": "Today, when we build a deep neural network with standard layers and activation functions, we don’t worry much about the initialization of network’s parameters. But, this wasn’t always so. Exploding/vanishing gradients and numerical instability issues were likely if you were not careful about initialization. We hardly encounter such problems nowadays, partly because ReLU has become de-facto non-linearity choice and modern neural network frameworks use the right initialization schemes under the hood. But, why is initialization important at all?\nWe can consider a neural network as a collection learnable parameters that are initialized randomly and updated with gradient descent. In theory, this must find the optimal parameters, no matter the initial values. However, since the floating point representations used in computers have a finite range and precision, picking wrong initial values may cause out-of-range numerical values and cripple training process.\nOK, how about constraining all the initial values to a specific range, such as [-1, 1] or [-0.1, 0.1]? Would it solve the problem? Unfortunately, it may not. In modern deep neural networks, the input goes through many layers, each changing incoming data in a specific way. Therefore, the changes accumulate through the network and eventually may cause the values to grow too large or too small. To solve this problem for all networks with an arbitrary number of layers, we use proven initialization schemes that keep the variance of data and gradients unchanged through the neural network as much as possible."
  },
  {
    "objectID": "posts/initialization-in-nn/index.html#how-does-the-properties-of-a-distribution-changes-through-various-transformations",
    "href": "posts/initialization-in-nn/index.html#how-does-the-properties-of-a-distribution-changes-through-various-transformations",
    "title": "Why is initialization important in neural networks?",
    "section": "How does the properties of a distribution changes through various transformations?",
    "text": "How does the properties of a distribution changes through various transformations?\n\n\nCode\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport seaborn\n\n# Set seaborn aesthetic parameters to defaults\nseaborn.set()\nseaborn.set_style('whitegrid')\nseaborn.set_palette(\"dark\")\n\nimport numpy as np\nimport torch\nnp.random.seed(42)\n\ndef plot_distributions(*arrays, bins=None, fig_width=12, fig_height=4, xlim=None):\n    n = len(arrays)\n    fig = plt.figure(figsize=(fig_width, fig_height*n))\n    for i, x in enumerate(arrays):\n        ax = fig.add_subplot(n, 1, i+1)\n        seaborn.distplot(x, bins=bins)\n        ax.set_title(stats_desc(x))\n        ax.set_xlim(xlim)\n\n\n\ndef plot_distribution(x, bins=None, fig_width=12, fig_height=4, xlim=None):\n    fig = plt.figure(figsize=(fig_width, fig_height))\n    seaborn.distplot(x, bins=bins)\n    plt.title(name + ' ' + stats_desc(x))\n    plt.xlim(xlim)\n\n\ndef stats_desc(x):\n    return f\"μ: {x.mean():+.3f}  σ: {x.std():.3f}\"\n\nLet’s assume that the input is from a uniform distribution between [-1, 1].\n\\(x \\sim U[-1, 1]\\)\n\nn = 1000\nx = np.random.uniform(-1, 1, n)\nplot_distributions(x, bins=100)\n\n\n\n\n\nSummation\nThe sum of two independent uniform distribution is a distribution with\n\\(\\quad \\mu= \\mu_1 + \\mu_2 \\quad \\sigma^2=\\sigma_1^2 + \\sigma_2^2\\)\n\ny = np.random.uniform(0, 10, n)\nz = x + y\n\nprint(f\"Added a uniform dist. {stats_desc(y)}\")\n\nplot_distributions(x, y, xlim=(-2, 15), fig_width=12)\n\nAdded a uniform dist. μ: +5.070  σ: 2.920\n\n\n\n\n\n\n\nScale and shift\n\ny = 2 * x + 1\n\nplot_distributions(x, y, xlim=(-4, 4))\n\n\n\n\n\n\nAffine transformation\nA linear layer (or fully connected layer) with weights, \\(W \\sim U[-c, c]\\) , approximately transforms a uniform distribution \\(x \\sim U[-1, 1]\\) to a normal distribution \\(y \\sim \\mathcal N(\\mu, \\sigma)\\)\n\\[ \\mu = 0 \\quad \\sigma^2 = n_{in} ~ \\sigma_{input}^2 ~ \\sigma_{w}^2 \\]\n\\(n_{in}\\): size of input, also denoted as, \\(\\text{fan}_{in}\\)\nLet’s initialize weights with \\(W \\sim U[-1, 1]\\).\n\nW = np.random.uniform(-1, 1, size=(n, n))\nbias = np.random.uniform(-1, 1, size=n)\n\nprint(f\"Weight {stats_desc(W)}\")\nprint(f\"Bias   {stats_desc(bias)}\")\n\ny = x @ W + bias\nplot_distributions(x, y, xlim=(-30, 30))\n\nWeight μ: +0.001  σ: 0.577\nBias   μ: +0.008  σ: 0.566\n\n\n\n\n\nThe variance increased dramatically even after one layer.\nLet’s try smaller weights with \\(W \\sim U[-0.1, 0.1]\\)\n\nW = np.random.uniform(-.1, .1, size=(n, n))\nbias = np.random.uniform(-.1, .1, size=n)\n\nprint(f\"Weight {stats_desc(W)}\")\nprint(f\"Bias   {stats_desc(bias)}\")\n\ny = x @ W + bias\nplot_distributions(x, y, xlim=(-4, 4))\n\nWeight μ: -0.000  σ: 0.058\nBias   μ: +0.002  σ: 0.057\n\n\n\n\n\nThis looks better, the variance goes up slightly. Let’s see how it performs with multiple layers with various sizes.\n\nclass Affine:\n    def __init__(self, dim_in, dim_out, weight_bound=0.1, bias_bound=0.1):\n        self.W = np.random.uniform(-weight_bound, weight_bound, size=(dim_in, dim_out))\n        self.b = np.random.uniform(-bias_bound, bias_bound, size=dim_out)\n\n    def __call__(self, x):\n        return x @ self.W + self.b\n\n\nlayers = [\n    Affine(x.shape[-1], 256),\n    Affine(256, 512),\n    Affine(512, 1024),\n    Affine(1024, 1024),\n    Affine(1024, 1024),\n    Affine(1024, 1024),\n]\n\n\nactivations = []\ny = x\nfor layer in layers:\n    y = layer(y)\n    activations.append(y)\n\n\nplot_distributions(x, *activations, xlim=(-25, 25))\n\n\n\n\nThe variance keeps increasing after each layer and at the end it is much larger than the beginning’s."
  },
  {
    "objectID": "posts/initialization-in-nn/index.html#solution",
    "href": "posts/initialization-in-nn/index.html#solution",
    "title": "Why is initialization important in neural networks?",
    "section": "Solution",
    "text": "Solution\nWe need to keep variance of data and gradients unchanged through the network as much as possible.\nIn forward pass,\n\\[ \\sigma^2 = n_{in} ~ \\sigma_{input}^2 ~ \\sigma_{w}^2 \\]\nTo keep variance unchanged, we want to have\n\\[ n_{in} ~ \\sigma_w^2 = 1 \\quad \\sigma_{w}^2 = \\frac{1}{n_{in}} \\]\nA similar tranformation happens in backward pass when gradients flow in reverse direction.\n\\[ \\sigma_\\text{grad before layer}^2 = n_{out}~\\sigma_{w}^2~\\sigma_{\\text{grad after layer}}^2 \\]\nAgain, we want to have\n\\[ n_{out} ~ \\sigma_w^2 = 1 \\quad \\sigma_{w}^2 = \\frac{1}{n_{out}} \\]\nUnless \\(n_{in}=n_{out}\\), we cannot satisfy both constraints. Therefore, we do our best.\n\\[\n\\frac{1}{2} (n_{in} + n_{out}) \\sigma_{w}^2 = 1 \\\\ \\sigma_w^2 = \\frac{2}{n_{in} + n_{out}}\n\\]\nRemember the variance of a uniform distribution with U[a, b]\n\\[ \\sigma^2 = \\frac{(b-a)^2}{12} \\]\n\\[ \\sigma_{w}^2 = \\frac{(2c)^2}{12} = \\frac{c^2}{3} \\]\nHence,\n\\[ \\frac{c^2}{3} = \\frac{2}{n_{in} + n_{out}} \\quad c = \\sqrt{\\frac{6}{n_{in} + n_{out}}} \\]\nThis is known as Xavier initialization.\nNow, let’s initialize the weights according to Xavier initialization, see how it affects variance.\n\nclass XavierAffine:\n    def __init__(self, dim_in, dim_out):\n        weight_bound = np.sqrt(6/(dim_in + dim_out))\n        bias_bound = np.sqrt(2/(dim_in + dim_out))\n        \n        self.W = np.random.uniform(-weight_bound, weight_bound, size=(dim_in, dim_out))\n        self.b = np.random.uniform(-bias_bound, bias_bound, size=dim_out)\n\n    def __call__(self, x):\n        return x @ self.W + self.b\n\n\nxav_layers = [\n    XavierAffine(x.shape[-1], 256),\n    XavierAffine(256, 512),\n    XavierAffine(512, 1024),\n    XavierAffine(1024, 1024),\n    XavierAffine(1024, 1024),\n    XavierAffine(1024, 1024),\n]\n\n\nactivations = []\ny = x\nfor layer in xav_layers:\n    y = layer(y)\n    activations.append(y)\n\n\nplot_distributions(x, *activations, xlim=(-2, 2))\n\n\n\n\nAs it’s seen, with Xaiver initialization, the variance does not vary much through the layers."
  },
  {
    "objectID": "posts/initialization-in-nn/index.html#references",
    "href": "posts/initialization-in-nn/index.html#references",
    "title": "Why is initialization important in neural networks?",
    "section": "References",
    "text": "References\n\nhttp://d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html#xavier-initialization"
  },
  {
    "objectID": "posts/halting-problem/index.html",
    "href": "posts/halting-problem/index.html",
    "title": "Halting problem",
    "section": "",
    "text": "Halting problem was proved to be unsolvable by Alan Turing in his seminal paper, in 1936. It states that there cannot be a general algorithm to decide whether an arbitrary pair of computer program and input halts (finishes execution) or not.\nSurprisingly, the proof of such a bold statement is ingeniously simple.\nProof by contradiction.\nSuppose that there exists a computer program H that solves halting problem. For a pair of computer program P and input I, H outputs true if P finished with I. Otherwise, it outputs false. How H can do this is not important for the sake of proof. In Python, H would be roughly such:\ndef H(program, inpt):\n    # with some black magic\n    if program_halts_on_inpt:\n        return True\n    else:\n        return False\nNow, having H, let’s define another computer program G, which receives a computer program P. G copies P and asks H whether P halts on P. If H decides that P halts on P, G diabolically loops forever. If H decides otherwise, G halts.\ndef G(program):\n    if H(program, program):\n        while True:\n            pass\n    else:\n        return\nNow, the interesting part. Let’s run G with G as input, i.e. call G(G). It calls H(G, G) and there are two possible outcomes. 1. H decides that program G halts on input G and returns true. Then, inside G, first brach of if becomes active and G loops forever, i.e. does not halt. 1. H decides that program G does not halt on input G and returns false. Then, inside G, else branch becomes active and G halts.\nThis is a contradiction. Hence, H cannot exist."
  },
  {
    "objectID": "posts/fastai-huggingface-datasets/index.html",
    "href": "posts/fastai-huggingface-datasets/index.html",
    "title": "How to use HuggingFace datasets with fastai?",
    "section": "",
    "text": "In this tutorial, we will show you how to use a HuggingFace dataset with fastai to train a model for image classification. We will use the Beans dataset, which consists of images of beans with three different types of diseases."
  },
  {
    "objectID": "posts/fastai-huggingface-datasets/index.html#step-1-install-the-required-libraries",
    "href": "posts/fastai-huggingface-datasets/index.html#step-1-install-the-required-libraries",
    "title": "How to use HuggingFace datasets with fastai?",
    "section": "Step 1: Install the required libraries",
    "text": "Step 1: Install the required libraries\nBefore starting, we need to install the required libraries. Run the following commands to install fastai and HuggingFace’s datasets:\n\n!pip install -Uqq fastai\n!pip install -Uqq datasets\n\nLogin to HuggingFace Hub to download the dataset.\n\n!huggingface-cli login"
  },
  {
    "objectID": "posts/fastai-huggingface-datasets/index.html#step-2-import-the-required-modules",
    "href": "posts/fastai-huggingface-datasets/index.html#step-2-import-the-required-modules",
    "title": "How to use HuggingFace datasets with fastai?",
    "section": "Step 2: Import the required modules",
    "text": "Step 2: Import the required modules\n\nimport torch\nfrom fastai.data.all import *\nfrom fastai.vision.all import *\nfrom datasets import load_dataset"
  },
  {
    "objectID": "posts/fastai-huggingface-datasets/index.html#step-3-load-the-dataset",
    "href": "posts/fastai-huggingface-datasets/index.html#step-3-load-the-dataset",
    "title": "How to use HuggingFace datasets with fastai?",
    "section": "Step 3: Load the dataset",
    "text": "Step 3: Load the dataset\nLet’s load Beans dataset, which is a dataset of images of beans taken in the field using smartphone cameras. It consists of 3 classes: 2 disease classes and the healthy class.\n\nraw_ds = load_dataset(\"beans\")\n\n\nraw_ds\n\nDatasetDict({\n    train: Dataset({\n        features: ['image_file_path', 'image', 'labels'],\n        num_rows: 1034\n    })\n    validation: Dataset({\n        features: ['image_file_path', 'image', 'labels'],\n        num_rows: 133\n    })\n    test: Dataset({\n        features: ['image_file_path', 'image', 'labels'],\n        num_rows: 128\n    })\n})\n\n\nThe dataset is splitted into train, validation, and test sets.\nLet’s see label names.\n\nclass_names = raw_ds['train'].features['labels'].names\nclass_names\n\n['angular_leaf_spot', 'bean_rust', 'healthy']"
  },
  {
    "objectID": "posts/fastai-huggingface-datasets/index.html#step-4-preprocess-the-dataset",
    "href": "posts/fastai-huggingface-datasets/index.html#step-4-preprocess-the-dataset",
    "title": "How to use HuggingFace datasets with fastai?",
    "section": "Step 4: Preprocess the dataset",
    "text": "Step 4: Preprocess the dataset\nOften, we need preprocessing raw data before training a model with it. HuggingFace datasets library provides two methods for preprocessing:\n\nmap: This method is used to apply a function to each example in the dataset, possibly in a batched manner. The function can be applied to one or more columns of the dataset, and the result can be stored in a new column or overwrite the existing one. The map function also allows you to remove some columns from the dataset, if needed. This method is useful for preprocessing the dataset, such as resizing images, tokenizing text, or encoding categorical features. It caches outputs so that they’re not computed again.\nset_transforms: This method is used to set a transform function that is applied on-the-fly when accessing examples from the dataset. This means that the dataset is not modified in-place, and the transform function is applied only when the examples are accessed. This method is useful for applying data augmentation techniques or normalization that should be applied dynamically during training without modifying the dataset beforehand.\n\nLet’s resize each image to 224x244.\n\ndef preprocess(records):\n    records[\"image\"] = [image.convert(\"RGB\").resize((224, 224)) for image in records[\"image\"]]\n    return records\n\nBefore batching samples, we need to remove unnecessary columns in the dataset such as image_file_path.\n\nds = raw_ds.map(preprocess, remove_columns=[\"image_file_path\"], batched=True)\n\nLoading cached processed dataset at /Users/bdsaglam/.cache/huggingface/datasets/beans/default/0.0.0/90c755fb6db1c0ccdad02e897a37969dbf070bed3755d4391e269ff70642d791/cache-d335def00fc26298.arrow\nLoading cached processed dataset at /Users/bdsaglam/.cache/huggingface/datasets/beans/default/0.0.0/90c755fb6db1c0ccdad02e897a37969dbf070bed3755d4391e269ff70642d791/cache-9b27f1e864a73628.arrow\nLoading cached processed dataset at /Users/bdsaglam/.cache/huggingface/datasets/beans/default/0.0.0/90c755fb6db1c0ccdad02e897a37969dbf070bed3755d4391e269ff70642d791/cache-417ffd63aef737e6.arrow\n\n\n\nds['train'][0]\n\n{'image': &lt;PIL.PngImagePlugin.PngImageFile image mode=RGB size=224x224&gt;,\n 'labels': 0}\n\n\nWe won’t use set_transform as fastai’s DataBlock can apply item-level and batch-level transforms, e.g. data augmentations, normalization. When we use a pretrained model, it already applies necessary transforms such as normalization."
  },
  {
    "objectID": "posts/fastai-huggingface-datasets/index.html#step-5-create-the-datablock",
    "href": "posts/fastai-huggingface-datasets/index.html#step-5-create-the-datablock",
    "title": "How to use HuggingFace datasets with fastai?",
    "section": "Step 5: Create the DataBlock",
    "text": "Step 5: Create the DataBlock\nNow, we can create dataloaders for the dataset using DataBlock from fastai. As the dataset is already splitted into train, validation, and test sets, we don’t need to split it further. Hence, we will use nosplit function.\n\ndef nosplit(items): \n    return list(range(len(items))), []\n\n\ndblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_x=lambda record: record['image'],\n    get_y=lambda record: class_names[record['labels']],\n    splitter = nosplit,\n)\n\ntrain_dl = dblock.dataloaders(ds['train']).train\nvalid_dl = dblock.dataloaders(ds['validation']).train\ndls = DataLoaders(train_dl, valid_dl)\n\ndls.show_batch()\n\n\n\n\n\nlen(dls.train.dataset),len(dls.valid.dataset)\n\n(1034, 133)"
  },
  {
    "objectID": "posts/fastai-huggingface-datasets/index.html#step-6-training",
    "href": "posts/fastai-huggingface-datasets/index.html#step-6-training",
    "title": "How to use HuggingFace datasets with fastai?",
    "section": "Step 6: Training",
    "text": "Step 6: Training\nLet’s fine-tune a pretrained ResNet model on our dataset using Learner.fine_tune method.\n\nlearn = vision_learner(\n    dls, \n    resnet34, \n    loss_func=CrossEntropyLossFlat(),\n    metrics=accuracy, \n)\n\n\n# Find a good learning rate\nlearn.lr_find()\n\nSuggestedLRs(valley=0.0004786300996784121)\n\n\n\n\n\n\n# Fine-tune the model\nlearn.fine_tune(1, 1e-3, freeze_epochs=2)"
  },
  {
    "objectID": "posts/fastai-huggingface-datasets/index.html#step-7-evaluation",
    "href": "posts/fastai-huggingface-datasets/index.html#step-7-evaluation",
    "title": "How to use HuggingFace datasets with fastai?",
    "section": "Step 7: Evaluation",
    "text": "Step 7: Evaluation\nOnly after 3 epochs, fine-tuned model achieves 90.6% accuracy on validation set, not too bad!\n\nloss, accuracy = learn.validate(dl=dls.valid)\nprint(f\"Loss {loss:.6f}\\nAccuracy: {accuracy:.2%}\")\n\nLoss 0.306838\nAccuracy: 90.62%\n\n\nLet’s check predictions visually.\n\nlearn.show_results(dl=valid_dl)\n\n\n\n\nLet’s predict and evaluate on test set.\n\ntst_dl = dls.test_dl(ds['test'], with_labels=True)\nprobs, targets, preds = learn.get_preds(dl=tst_dl, with_decoded=True)\n\n\nloss, accuracy = learn.validate(dl=tst_dl)\nprint(f\"Loss {loss:.6f}\\nAccuracy: {accuracy:.2%}\")\n\nLoss 0.288472\nAccuracy: 89.06%"
  }
]