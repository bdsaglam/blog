{
  
    
        "post0": {
            "title": "How to use a custom model with fastai cnn_learner?",
            "content": "fastai&#39;s cnn_learner performs several actions to create a model from given a pretrained model architecture such as resnet18. First, it gets model meta from model_meta registry. model_meta registry contains model meta data such as dataset statistics that the model is pre-trained on, the index to split the network into backbone and head. For instance, . def _xresnet_split(m): return L(m[0][:3], m[0][3:], m[1:]).map(params) model_meta = { models.xresnet.xresnet18 :{&#39;cut&#39;:-4, &#39;split&#39;:_xresnet_split, &#39;stats&#39;:imagenet_stats}, ... } . The cut value is used for stripping off the existing classification head of the network so that we can add a custom head and fine-tune it for our task. The split function is used when discriminative learning rate schema is applied such that the layers of a model are trained with different learning rates. The stats refer to the channel means and standard deviations of the images in ImageNet dataset, which the model is pretrained on. Many CNN architectures are already registered in fastai. . There are two alternative ways to to use a custom model not present in model registry: . Create a new helper function similar to cnn_learner that splits the network into backbone and head. | Register the architecture in model_meta. | We prefer the second way since it is easier and shorter. . Let&#39;s first inspect an architecture registered already, e.g. resnet18. . Here is its model meta data from the registry: . from fastai.vision.all import * model_meta[resnet18] . {&#39;cut&#39;: -2, &#39;split&#39;: &lt;function fastai.vision.learner._resnet_split(m)&gt;, &#39;stats&#39;: ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])} . And the model layers: . m = resnet18() children = list(m.children()) print(f&#39;The network has {len(children)} children&#39;) children . The network has 10 children . [Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False), Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ), Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ), Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ), Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ), AdaptiveAvgPool2d(output_size=(1, 1)), Linear(in_features=512, out_features=1000, bias=True)] . create_body function called by create_cnn_model which is called in cnn_learner, strips off the head by cut index as such: . ... if isinstance(cut, int): return nn.Sequential(*list(model.children())[:cut]) ... . In our case, it&#39;ll remove the last two layers of resnet18 network: AdaptiveAvgPool2d and fully connected Linear layer. . TODO: Why does it remove pooling layer? . body = create_body(resnet18, pretrained=False, cut=model_meta[resnet18][&#39;cut&#39;]) body . Sequential( (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (5): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (6): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (7): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) . Similarly, we need to determine the cut index for the custom model we use. Let&#39;s try EfficientNetB0 architecture available in torchvision library. First, we inspect the network layers to find out where to split it into backbone and head. . from torchvision.models import efficientnet_b0 m = efficientnet_b0() children = list(m.children()) print(f&#39;The network has {len(children)} children&#39;) children . The network has 3 children . [Sequential( (0): ConvNormActivation( (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): Sequential( (0): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (2): ConvNormActivation( (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.0, mode=row) ) ) (2): Sequential( (0): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.0125, mode=row) ) (1): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False) (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.025, mode=row) ) ) (3): Sequential( (0): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False) (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row) ) (1): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False) (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.05, mode=row) ) ) (4): Sequential( (0): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False) (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.0625, mode=row) ) (1): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False) (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row) ) (2): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False) (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row) ) ) (5): Sequential( (0): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False) (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.1, mode=row) ) (1): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False) (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.1125, mode=row) ) (2): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False) (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.125, mode=row) ) ) (6): Sequential( (0): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False) (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.1375, mode=row) ) (1): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False) (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row) ) (2): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False) (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.1625, mode=row) ) (3): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False) (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row) ) ) (7): Sequential( (0): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False) (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.1875, mode=row) ) ) (8): ConvNormActivation( (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) ), AdaptiveAvgPool2d(output_size=1), Sequential( (0): Dropout(p=0.2, inplace=True) (1): Linear(in_features=1280, out_features=1000, bias=True) )] . As it can be seen, the pooling layer is at index -2, which corresponds to the cut value. We&#39;ll use the default_split for split function and ImageNet stats as the model is pre-trained on it. . from fastai.vision.learner import default_split model_meta[efficientnet_b0] = {&#39;cut&#39;: -2, &#39;split&#39;: default_split, &#39;stats&#39;: imagenet_stats} . Now we can create a cnn_learner since our custom architecture is registered. . path = untar_data(URLs.PETS) files = get_image_files(path/&quot;images&quot;) def label_func(f): return f[0].isupper() dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(224)) learn = cnn_learner(dls, arch=efficientnet_b0) . Let&#39;s verify that the body and head are created correctly. . Body: . list(learn.model.children())[:-1] . [Sequential( (0): Sequential( (0): ConvNormActivation( (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): Sequential( (0): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (2): ConvNormActivation( (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.0, mode=row) ) ) (2): Sequential( (0): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.0125, mode=row) ) (1): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False) (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.025, mode=row) ) ) (3): Sequential( (0): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False) (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row) ) (1): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False) (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.05, mode=row) ) ) (4): Sequential( (0): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False) (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.0625, mode=row) ) (1): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False) (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row) ) (2): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False) (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row) ) ) (5): Sequential( (0): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False) (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.1, mode=row) ) (1): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False) (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.1125, mode=row) ) (2): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False) (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.125, mode=row) ) ) (6): Sequential( (0): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False) (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.1375, mode=row) ) (1): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False) (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row) ) (2): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False) (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.1625, mode=row) ) (3): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False) (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row) ) ) (7): Sequential( (0): MBConv( (block): Sequential( (0): ConvNormActivation( (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (1): ConvNormActivation( (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False) (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) (2): SqueezeExcitation( (avgpool): AdaptiveAvgPool2d(output_size=1) (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1)) (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1)) (activation): SiLU(inplace=True) (scale_activation): Sigmoid() ) (3): ConvNormActivation( (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (stochastic_depth): StochasticDepth(p=0.1875, mode=row) ) ) (8): ConvNormActivation( (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): SiLU(inplace=True) ) ) )] . Head: . list(learn.model.children())[-1] . Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.25, inplace=False) (4): Linear(in_features=2560, out_features=512, bias=False) (5): ReLU(inplace=True) (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.5, inplace=False) (8): Linear(in_features=512, out_features=2, bias=False) ) . As it&#39;s seen, cnn_learner created a new head starting with a pooling layer while keeping the backbone from the pre-trained model. . m.children? . Signature: m.children() -&gt; Iterator[ForwardRef(&#39;Module&#39;)] Docstring: Returns an iterator over immediate children modules. Yields: Module: a child module File: ~/miniconda3/envs/fastnote/lib/python3.8/site-packages/torch/nn/modules/module.py Type: method .",
            "url": "https://bdsaglam.github.io/blog/fastai/2022/02/10/fastai-custom-model.html",
            "relUrl": "/fastai/2022/02/10/fastai-custom-model.html",
            "date": " • Feb 10, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Halting problem",
            "content": "Halting problem was proved to be unsolvable by Alan Turing in his seminal paper, in 1936. It states that there cannot be a general algorithm to decide whether an arbitrary pair of computer program and input halts (finishes execution) or not. . Surprisingly, the proof of such a bold statement is ingeniously simple. . Proof by contradiction. . Suppose that there exists a computer program H that solves halting problem. For a pair of computer program P and input I, H outputs true if P finished with I. Otherwise, it outputs false. How H can do this is not important for the sake of proof. In Python, H would be roughly such: . def H(program, inpt): # with some black magic if program_halts_on_inpt: return True else: return False . Now, having H, let&#39;s define another computer program G, which receives a computer program P. G copies P and asks H whether P halts on P. If H decides that P halts on P, G diabolically loops forever. If H decides otherwise, G halts. . def G(program): if H(program, program): while True: pass else: return . Now, the interesting part. Let&#39;s run G with G as input, i.e. call G(G). It calls H(G, G) and there are two possible outcomes. . H decides that program G halts on input G and returns true. Then, inside G, first brach of if becomes active and G loops forever, i.e. does not halt. | H decides that program G does not halt on input G and returns false. Then, inside G, else branch becomes active and G halts. | This is a contradiction. Hence, H cannot exist. .",
            "url": "https://bdsaglam.github.io/blog/computer-science/2020/08/07/halting-problem.html",
            "relUrl": "/computer-science/2020/08/07/halting-problem.html",
            "date": " • Aug 7, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "How many coin flips does high confidence need?",
            "content": "#collapse-hide %matplotlib inline import matplotlib.pyplot as plt import seaborn # Set seaborn aesthetic parameters to defaults seaborn.set() seaborn.set_style(&#39;whitegrid&#39;) seaborn.set_palette(&quot;dark&quot;) import math import numpy as np from scipy.special import comb np.random.seed(42) . . You walked into a casino, there is a game of tossing coins. You don&#39;t know whether the coin is biased or fair. But, you can observe the game and keep the stats. How many tosses would you observe to measure the probability of the coin? . Though it is not possible to exactly determine the underlying probability distribution of the coin, we can measure in high confidence, if we observe enough tosses. . The probability distribution of a coin toss is known as Bernoulli distribution parameterized with p, the probability of heads. There is no way we can directly measure this. Instead, we count the heads, $k$, and divide it into total number of tosses, $n$. How do we know that this is a good way of measuring p? Well, the probability distribution of number of heads is Binomial distribution, parameterized with p and n. And the expectation of Bernoulli distribution is $np$. Therefore, if we count heads and divide it by total number of tosses, we have an estimate of $p$. . $$ text{Binomial Distribution } $$$$ P_{p,n}(k: text{#heads}) = binom{N}{k} cdot p^k cdot (1-p)^{n-k} $$$$ mathbb{E}[ #heads]= sum limits_{i=1}^n mathbb{E}[ mathbb{1}[coin_i]] = sum limits_{i=1}^n p = np tag*{(Linearity of Expectation) } $$$$ mathbb{1}[coin_i] = 1 hspace{35pt} text{if ith coin comes heads} 0 hspace{48pt} text{otherwise} $$ Let&#39;s simulate this for 100 tosses. First, let&#39;s randomly pick $p$ and see what our estimation ends up. . n = 100 p = np.random.rand(1)[0] tosses = np.random.rand(n) heads = tosses &lt; p space = np.array([&#39;Heads&#39;, &#39;Tails&#39;]) print(space[heads.astype(np.int)]) print() p_est = heads.sum() / n error = abs(p - p_est) print(f&quot;True p t t: {p}&quot;) print(f&quot;Estimated p t: {p_est}&quot;) print(f&quot;Error t t: {error:%}&quot;) . [&#39;Heads&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Tails&#39; &#39;Tails&#39; &#39;Tails&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Tails&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Tails&#39; &#39;Tails&#39; &#39;Tails&#39; &#39;Tails&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Tails&#39; &#39;Heads&#39; &#39;Tails&#39; &#39;Tails&#39; &#39;Tails&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Tails&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Tails&#39; &#39;Heads&#39; &#39;Tails&#39; &#39;Tails&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Tails&#39; &#39;Tails&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Tails&#39; &#39;Heads&#39; &#39;Tails&#39; &#39;Heads&#39; &#39;Tails&#39; &#39;Heads&#39; &#39;Tails&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Tails&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Tails&#39; &#39;Tails&#39; &#39;Tails&#39; &#39;Tails&#39; &#39;Heads&#39; &#39;Tails&#39; &#39;Heads&#39; &#39;Tails&#39; &#39;Tails&#39; &#39;Heads&#39; &#39;Tails&#39; &#39;Heads&#39; &#39;Tails&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Tails&#39; &#39;Tails&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Tails&#39; &#39;Tails&#39; &#39;Tails&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Tails&#39; &#39;Tails&#39; &#39;Tails&#39; &#39;Tails&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Tails&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Heads&#39; &#39;Tails&#39; &#39;Tails&#39; &#39;Tails&#39;] True p : 0.3745401188473625 Estimated p : 0.45 Error : 7.545988% . 7% error! Way too much. . Let&#39;s repeat this experiment for different sample sizes and see how the error changes. . def measure(p, n): tosses = np.random.rand(n) heads = tosses &lt; p p_est = heads.sum() / n return p_est . sizes = np.array([2**i for i in range(1, 15)]) estimates = np.array([measure(p, int(size)) for size in sizes]) errors = np.abs(estimates - p) . plt.figure() plt.scatter(sizes, errors) plt.xscale(&#39;log&#39;) . Though there are some deviations, it seem&#39;s like error decreases as the sample size grows. But, how to know where to stop? Also, how confident are we about our measurements considering the deviations in them? Fortunately, with the help of probability theory, we can determine the number of samples required to reach the desired confidence level for our estimate. . Confidence Intervals . We want to ensure that our estimate is close to true value of $p$. In other words, we want the probability of the event that our estimate is within a small interval around $p$ as high as possible. This probability, a.k.a. confidence level, can be formulated as such, . $$ text{confidence level} = P[|p_{est} - p| leq text{interval}] = 1 - P[|p_{est} - p| &gt; text{interval}] = 1 - P[p_{est} &gt; p + text{interval} hspace{4pt} cup hspace{4pt} p_{est} &lt; p - text{interval}] = 1 - P[ frac{k}{n} &gt; p + text{interval} hspace{4pt} cup hspace{4pt} frac{k}{n} &lt; p - text{interval}] = 1 - (P[k &gt; n (p + text{interval})] + P[k &lt; n (p - text{interval})]) $$ $$ P[k &gt; n (p + text{interval})] = P[n-k &lt; n - n (p + text{interval})] = P[n-k &lt; n (1 - p - text{interval}))] $$ $$ text{confidence level} = 1 - (P[n-k &lt; n (1 - p - text{interval}))] + P[k &lt; n (p - text{interval})]) = 1 - ( F_{n,1-p}[n(1 - p - text{interval})] + F_{n,p}[n(p - text{interval})] ) F: text{Cumulative Distribution Function} $$ So, we need to calculate cumulative mass function (CMF) of Binomial distribution parameterized with $p$ and $n$. But, we don&#39;t know p. It turns out that the above expression takes it minimum value at $p=0.5$. Therefore, we can find a lower bound on confidence level with $p=0.5$. The formula for confidence level becomes, . $$ text{confidence level} &gt;= 1 - ( F_{n,0.5}[n(0.5 - text{interval})] + F_{n,0.5}[n(0.5 - text{interval})] ) = 1 - 2 F_{n,0.5}[n(0.5 - text{interval})] $$Since we know the probability mass function (PMF) of Binomial Distribution, it is easy to calculate CMF. . $$ F_x[X] = P_x[x &lt; X] = sum limits_{i=0}^{X-1} P[x=i] $$ Let&#39;s define PMF by using scipy.special.comb for combination function. . from scipy.special import comb . def binomial_pmf_naive(n, p, k): return comb(n, k) * p ** k * (1-p)**(n-k) . binomial_pmf_naive(2, 0.5, 0) . 0.25 . binomial_pmf_naive(100, .5, 50) . 0.07958923738717873 . binomial_pmf_naive(2000, .5, 1000) . inf . As you see, for big n values, Scipy&#39;s comb method overflows since it is out of bounds of Python&#39;s floating point number. To overcome this, we can use an approximation for log of combination function. . $$ binom{N}{k} sim frac{2^{nH(k/n)}}{ sqrt{2 pi k (1 - frac{k}{n})}} $$$$ H(a) = a log frac{1}{a} + (1-a) log frac{1}{1-a} tag*{[Entropy function]} $$ def entropy_function(a, eps=1e-32): return a * np.log2(1 / (a + eps)) + (1 - a) * np.log2(1 / (1 - a + eps)) . #collapse-hide x = np.linspace(0, 1, 100) y = entropy_function(x) fig, ax = plt.subplots() ax.plot(x, y) ax.set_title(&quot;Entropy function&quot;); . . @np.vectorize def log_comb_approximate(n, k, eps=1e-32): if k==0 or k==n: return float(&quot;-inf&quot;) if k==1 or k==(n-1): return np.log(n) a = k/n return n * entropy_function(a) * np.log(2) - np.log(np.sqrt(2 * np.pi * a * (1-a) * n) + eps) . #collapse-hide n = 1000 plt.figure(figsize=(8, 4)) plt.plot(comb(n, np.arange(n+1)), color=&#39;blue&#39;) plt.plot(np.exp(log_comb_approximate(n, np.arange(n+1))).T, color=&#39;red&#39;) plt.legend([&#39;scipy comb&#39;, &#39;approx. comb&#39;]); . . #collapse-hide n = 100 k = np.arange(n) expected = comb(n, k) approximate = np.exp(log_comb_approximate(n, k)) eps = 1e-8 rel_error = np.abs(approximate/(expected+eps) - 1) fig, ax = plt.subplots(figsize=(8, 4)) ax.semilogy(k, rel_error) ax.set_title(&#39;Combination function approximation error&#39;); . . OK, the approximation seems pretty close. Now, we can define PMF. . def binomial_pmf(n, p, k, eps=1e-32): if isinstance(p, (float, int)): p = [p] if isinstance(k, (float, int)): k = [k] p = np.array(p)[:, None] k = np.array(k)[None, :] return np.exp(log_comb_approximate(n, k) + k * np.log(p + eps) + (n - k) * np.log(1 - p + eps)) . #collapse-hide n = 2000 p = 0.4 k = np.arange(n+1) pmf = binomial_pmf(n, p, k) plt.figure(figsize=(12, 6)) plt.plot(k, pmf.T) plt.xlabel(&quot;Number of heads&quot;) plt.ylabel(&quot;Probability Mass Function&quot;) plt.title(f&quot;Binomial Distribution with p={p}&quot;); . . Let&#39;s sanity-check approximated PMF. . print(pmf.sum()) . 1.0001320546143024 . CMF is just cumulative sum of PMF. . #collapse-hide cmf = pmf.cumsum(-1) plt.figure(figsize=(12, 6)) plt.plot(k, cmf.T) plt.xlabel(&quot;Number of heads&quot;) plt.ylabel(&quot;Cumulative Mass Function&quot;) plt.title(f&quot;Binomial Distribution with p={p}&quot;); . . Now, let&#39;s define a function that calculates confidence level for given sample size and interval. . $$ text{confidence level} geq 1 - 2 F_{n,0.5}[n(0.5 - text{interval})]$$ . def compute_confidence(n, interval): p = 1/2 low = np.ceil(n*(p-interval)).astype(np.int) pmf = binomial_pmf(n, p, k=np.arange(low+1)).squeeze() cmf = pmf.cumsum() return 1 - 2 * cmf[low] . Let&#39;s say we want a tight interval around true value of $p$, so we set it 0.01. . interval = 0.01 . And for sample size of $n=100$, the minimum confidence becomes, . n = 100 conf = compute_confidence(n, interval) print(f&quot;confidence level &gt;{conf: %}&quot;) . confidence level &gt; 7.725091% . This is very low, we definitely need more samples. Let&#39;s go for $n=1000$. . n=1000 conf = compute_confidence(n, interval) print(f&quot;confidence level &gt;{conf: %}&quot;) . confidence level &gt; 45.188993% . It gets better, but we cannot keep trying all values one by one. Let&#39;s plot confidence level against different sample sizes. . #collapse-hide import matplotlib.ticker as mtick ns = np.power(2, np.arange(6, 18)) # sample sizes confs = np.array([compute_confidence(n, interval) for n in ns]).squeeze() plt.figure(figsize=(12, 8)) plt.semilogx(ns, confs*100) plt.ylabel(&quot;Confidence Level&quot;) plt.xlabel(&#39;Sample size&#39;) plt.title(f&quot;interval={interval}&quot;) plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter()) . . We need more than 10,000 samples to have confident estimate of $p$. . Let&#39;s see how it changes with different interval values. . #collapse-hide intervals = np.arange(1, 5) / 100 ns = np.power(2, np.arange(6, 17)) # sample sizes confs = np.array([[compute_confidence(n, interval) for n in ns] for interval in intervals]).squeeze() plt.figure(figsize=(12, 8)) plt.semilogx(ns, confs.T*100, lw=3) plt.ylabel(&quot;Confidence Level&quot;) plt.xlabel(&#39;Sample size&#39;) plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter()) plt.legend([f&#39;{interval}&#39; for interval in intervals]); . . As interval gets tighter, we need more samples for a confidence level. Now, let&#39;s write a function that calculates required sample size for given interval and confidence level. . #collapse-hide plt.figure(figsize=(12, 8)) plt.semilogy(ns, (1 - confs.T)*100, lw=3) plt.ylabel(&quot;Error Level&quot;) plt.xlabel(&#39;Sample size&#39;) plt.legend([f&#39;{interval}&#39; for interval in intervals]); . . The error ($1 - text{confidence level}$) gets exponentially smaller as the sample size increases. Hence, we fit a 1D spline linear interpolator on logarithm of error with respect to sample size. . from scipy.interpolate import interp1d def find_sample_size(confidence, interval, p=1/2, eps=1e-32): assert interval &gt;= .01 ns = np.power(2, np.arange(1, 18)) confs = np.array([compute_confidence(n, interval) for n in ns]).squeeze() confs = np.clip(confs, 0, 1) inconfs = 1 - confs f = interp1d(np.log(inconfs + eps), ns, kind=&#39;slinear&#39;) return f(np.log(1-confidence)) . Let&#39;s check for various confidence levels. . Min sample size for conf=0.95, interval=0.04 is 659 . Min sample size for conf=0.95, interval=0.01 is 9943 . Min sample size for conf=0.999999, interval=0.01 is 60058 . Conclusion . Let&#39;s say we tossed the coin for 659 times and the number of heads is 309. So, our estimate for $p$ is 0.47. Does this mean $p$ is likely to be in 0.47 ± 0.04? No, quite the opposite. The probability of heads is not probabilistic, it is a characteristic of the coin. We are trying to measure it. On the contrary, our estimate is probabilistic as we take finite number of samples to derive it. Hence, we can only talk about the likelihood of the estimate being close to true value. Specifically, the confidence interval states that there is 95% chance that the estimate is within ±4% interval of true value. . This method can be used to determine number of samples required for polls, if we assume that each individual&#39;s answers are mutually independent. The good news is the sample size required does not depend on population size and confidence level gets exponentially better w.r.t. sample size. . References . Eric Lehman, F. Thomson Leighton, Albert R. Meyer. Mathematics for Computer Science. Lecture Notes, available at https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-fall-2010/readings/MIT6_042JF10_notes.pdf, 2010. |",
            "url": "https://bdsaglam.github.io/blog/math/2020/07/25/probability-confidence.html",
            "relUrl": "/math/2020/07/25/probability-confidence.html",
            "date": " • Jul 25, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Why initialization is important in neural networks?",
            "content": "Today, when we build a deep neural network with standard layers and activation functions, we don&#39;t worry much about the initialization of network&#39;s parameters. But, this wasn&#39;t always so. Exploding/vanishing gradients and numerical instability issues were likely if you were not careful about initialization. We hardly encounter such problems nowadays, partly because ReLU has become de-facto non-linearity choice and modern neural network frameworks use the right initialization schemes under the hood. But, why is initialization important at all? . We can consider a neural network as a collection learnable parameters that are initialized randomly and updated with gradient descent. In theory, this must find the optimal parameters, no matter the initial values. However, since the floating point representations used in computers have a finite range and precision, picking wrong initial values may cause out-of-range numerical values and cripple training process. . OK, how about constraining all the initial values to a specific range, such as [-1, 1] or [-0.1, 0.1]? Would it solve the problem? Unfortunately, it may not. In modern deep neural networks, the input goes through many layers, each changing incoming data in a specific way. Therefore, the changes accumulate through the network and eventually may cause the values to grow too large or too small. To solve this problem for all networks with an arbitrary number of layers, we use proven initialization schemes that keep the variance of data and gradients unchanged through the neural network as much as possible. . How does the properties of a distribution changes through various transformations? . #collapse-hide %matplotlib inline import matplotlib.pyplot as plt import seaborn # Set seaborn aesthetic parameters to defaults seaborn.set() seaborn.set_style(&#39;whitegrid&#39;) seaborn.set_palette(&quot;dark&quot;) import numpy as np import torch np.random.seed(42) def plot_distributions(*arrays, bins=None, fig_width=12, fig_height=4, xlim=None): n = len(arrays) fig = plt.figure(figsize=(fig_width, fig_height*n)) for i, x in enumerate(arrays): ax = fig.add_subplot(n, 1, i+1) seaborn.distplot(x, bins=bins) ax.set_title(stats_desc(x)) ax.set_xlim(xlim) . . def plot_distribution(x, bins=None, fig_width=12, fig_height=4, xlim=None): fig = plt.figure(figsize=(fig_width, fig_height)) seaborn.distplot(x, bins=bins) plt.title(name + &#39; &#39; + stats_desc(x)) plt.xlim(xlim) . def stats_desc(x): return f&quot;μ: {x.mean():+.3f} σ: {x.std():.3f}&quot; . Let&#39;s assume that the input is from a uniform distribution between [-1, 1]. . $x sim U[-1, 1]$ . n = 1000 x = np.random.uniform(-1, 1, n) plot_distributions(x, bins=100) . Summation . The sum of two independent uniform distribution is a distribution with . $ quad mu= mu_1 + mu_2 quad sigma^2= sigma_1^2 + sigma_2^2$ . y = np.random.uniform(0, 10, n) z = x + y print(f&quot;Added a uniform dist. {stats_desc(y)}&quot;) plot_distributions(x, y, xlim=(-2, 15), fig_width=12) . Added a uniform dist. μ: +5.070 σ: 2.920 . Scale and shift . y = 2 * x + 1 plot_distributions(x, y, xlim=(-4, 4)) . Affine transformation . A linear layer (or fully connected layer) with weights, $W sim U[-c, c]$ , approximately transforms a uniform distribution $x sim U[-1, 1]$ to a normal distribution $y sim mathcal N( mu, sigma)$ . $$ mu = 0 quad sigma^2 = n_{in} ~ sigma_{input}^2 ~ sigma_{w}^2 $$ . $n_{in}$: size of input, also denoted as, $ text{fan}_{in}$ . Let&#39;s initialize weights with $W sim U[-1, 1]$. . W = np.random.uniform(-1, 1, size=(n, n)) bias = np.random.uniform(-1, 1, size=n) print(f&quot;Weight {stats_desc(W)}&quot;) print(f&quot;Bias {stats_desc(bias)}&quot;) y = x @ W + bias plot_distributions(x, y, xlim=(-30, 30)) . Weight μ: +0.001 σ: 0.577 Bias μ: +0.008 σ: 0.566 . The variance increased dramatically even after one layer. . Let&#39;s try smaller weights with $W sim U[-0.1, 0.1]$ . W = np.random.uniform(-.1, .1, size=(n, n)) bias = np.random.uniform(-.1, .1, size=n) print(f&quot;Weight {stats_desc(W)}&quot;) print(f&quot;Bias {stats_desc(bias)}&quot;) y = x @ W + bias plot_distributions(x, y, xlim=(-4, 4)) . Weight μ: -0.000 σ: 0.058 Bias μ: +0.002 σ: 0.057 . This looks better, the variance goes up slightly. Let&#39;s see how it performs with multiple layers with various sizes. . class Affine: def __init__(self, dim_in, dim_out, weight_bound=0.1, bias_bound=0.1): self.W = np.random.uniform(-weight_bound, weight_bound, size=(dim_in, dim_out)) self.b = np.random.uniform(-bias_bound, bias_bound, size=dim_out) def __call__(self, x): return x @ self.W + self.b . layers = [ Affine(x.shape[-1], 256), Affine(256, 512), Affine(512, 1024), Affine(1024, 1024), Affine(1024, 1024), Affine(1024, 1024), ] . activations = [] y = x for layer in layers: y = layer(y) activations.append(y) . plot_distributions(x, *activations, xlim=(-25, 25)) . The variance keeps increasing after each layer and at the end it is much larger than the beginning&#39;s. . Solution . We need to keep variance of data and gradients unchanged through the network as much as possible. . In forward pass, . $$ sigma^2 = n_{in} ~ sigma_{input}^2 ~ sigma_{w}^2 $$ . To keep variance unchanged, we want to have . $$ n_{in} ~ sigma_w^2 = 1 quad sigma_{w}^2 = frac{1}{n_{in}} $$ . A similar tranformation happens in backward pass when gradients flow in reverse direction. . $$ sigma_ text{grad before layer}^2 = n_{out}~ sigma_{w}^2~ sigma_{ text{grad after layer}}^2 $$ . Again, we want to have . $$ n_{out} ~ sigma_w^2 = 1 quad sigma_{w}^2 = frac{1}{n_{out}} $$ . Unless $n_{in}=n_{out}$, we cannot satisfy both constraints. Therefore, we do our best. . $$ frac{1}{2} (n_{in} + n_{out}) sigma_{w}^2 = 1 sigma_w^2 = frac{2}{n_{in} + n_{out}} $$Remember the variance of a uniform distribution with U[a, b] . $$ sigma^2 = frac{(b-a)^2}{12} $$ . $$ sigma_{w}^2 = frac{(2c)^2}{12} = frac{c^2}{3} $$ . Hence, . $$ frac{c^2}{3} = frac{2}{n_{in} + n_{out}} quad c = sqrt{ frac{6}{n_{in} + n_{out}}} $$ . This is known as Xavier initialization. . Now, let&#39;s initialize the weights according to Xavier initialization, see how it affects variance. . class XavierAffine: def __init__(self, dim_in, dim_out): weight_bound = np.sqrt(6/(dim_in + dim_out)) bias_bound = np.sqrt(2/(dim_in + dim_out)) self.W = np.random.uniform(-weight_bound, weight_bound, size=(dim_in, dim_out)) self.b = np.random.uniform(-bias_bound, bias_bound, size=dim_out) def __call__(self, x): return x @ self.W + self.b . xav_layers = [ XavierAffine(x.shape[-1], 256), XavierAffine(256, 512), XavierAffine(512, 1024), XavierAffine(1024, 1024), XavierAffine(1024, 1024), XavierAffine(1024, 1024), ] . activations = [] y = x for layer in xav_layers: y = layer(y) activations.append(y) . plot_distributions(x, *activations, xlim=(-2, 2)) . As it&#39;s seen, with Xaiver initialization, the variance does not vary much through the layers. . References . http://d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html#xavier-initialization |",
            "url": "https://bdsaglam.github.io/blog/neural-network/2020/06/25/initialization-in-nn.html",
            "relUrl": "/neural-network/2020/06/25/initialization-in-nn.html",
            "date": " • Jun 25, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Fourier Transform",
            "content": "Fourier transform decomposes a signal into the sinusoidal components with different amplitude, frequency and phases such that the sum of those components equals to the signal. It transforms a signal from time domain to frequency domain. Fourier transform is applicable to both continuous and discrete signals. In this post, we will only cover the discrete case. . For a signal with N points, Discrete Fourier Transform uses the following bases: . $e^{i~2 pi~k} = cos (2 pi~k)+i sin (2 pi~k)$ . for k = -N/2 ... N/2 where k=1 means 1 cycle per signal. . Nyquist&#39;s Sampling Theorem . To be able to measure a sinusoidal wave, we need to sample at least two points within its one full cycle, i.e., one point in half cycle. Therefore, we cannot measure the components that make more than N/2 cycles within N timesteps. This is known as Nyquist&#39;s sampling theorem. In practice, because of the risk of sampling those two points near zero crossings, we are only confident about the components with frequencies lower than N/4. . As an example, let&#39;s see the measurements from 1 Hz sine wave with various sampling rates. Nyquist&#39;s sampling theorem requires at least 2 Hz sampling rate for this signal. As you can see in the figure below, the sampling rate of 1 Hz (blue) measures a flat line. While sampling rate of 2 Hz (orange) is theoretically sufficent, its amplitude is far from the original&#39;s. As the sampling rate increases, the measurement becomes more accurate. . %matplotlib inline import matplotlib.pyplot as plt import numpy as np . # Original signal w = 1 # frequency of sine wave, [Hz] T = 3 # duration of signal [second] fs = 128 # underlying sampling rate of signal[Hz] t = np.linspace(0, T, fs * T) y = np.sin(2*np.pi*w*t) indices = np.arange(len(t)) fs_list = [1, 2, 4, 10] samples = [] for f in fs_list: offset = np.random.randint(0, 10) idx = indices[:-offset:fs//f] + offset t_sampled = t[idx] y_sampled = y[idx] samples.append((f, t_sampled, y_sampled)) . #collapse-hide plt.figure(figsize=(12, 8)) plt.plot(t, y, color=&#39;magenta&#39;, lw=4); legends = [&#39;Original signal&#39;] for f, ts, ys in samples: plt.plot(ts, ys, marker=&#39;o&#39;, ms=10, linestyle=&#39;--&#39;, lw=4); legends.append(f&#39;Fsamp = {f}ω&#39;) plt.legend(legends, loc=&#39;lower left&#39;) plt.xlim([0, 2]); . . Discrete Fourier Transform . Discrete Fourier Transform . $ X_k = sum_{n=0}^{N-1} x_n cdot e^{-i~2 pi~k~n~/~N} $ . Inverse Discrete Fourier Transform . $ x_n = frac{1}{N} sum_{k=0}^{N-1} X_k e^{i~2 pi~k~n~/~N} $ . The intuition behind first formula is that kth cosine component takes k/N cycles in 1 timestep or point interval. In other words, its angular velocity is 2πk/N radians/timestep. To make kth component&#39;s peak align with nth point, we need adjust the phase of kth component. Since it takes 2πk/N radians in one timestep, until nth point, it takes 2πkn/N radians. Therefore, we need to delay this component for 2πkn/N radians. To delay, we need to subtract from its phase, which means rotating its complex representation in negative/counter-clock-wise direction. Hence, there is a minus sign in front. . From the above formula, for N point signal, a naive Fourier transform algorithm has O(N2) time complexity. However, Cooley and Tukey (1965) proposed an O(N logN) algorithm; hence, named as Fast Fourier Transform. It benefits the symmetry in the problem and uses recursive divide-conquer approach. Checkout this detailed explanation of FFT algorithm, if you&#39;re interested. . Numpy provides FFT algorithm in numpy.fft subpackage along with some utilities. . Let&#39;s have a signal consisting of two sinusoidal waves with 5 Hz and 10 Hz and uniform noise. Since FFT makes recursive calls to divide the signal into two halves, the number of points in the signal must be power of 2. . fs = 128 # sampling rate, [Hz] T = 1 # duration of signal, [second] t = np.linspace(0, T, fs) N = len(t) . components = [ (1, 5, np.pi/4), # amplitude, frequency [Hz], phase [radian] (2, 10, 0), ] signal = sum([a * np.cos(2*np.pi*w*t + p) for (a, w, p) in components]) noise = 1*np.random.uniform(-1, 1, N) x = signal + noise . #collapse-hide plt.figure(figsize=(12, 6)) plt.plot(t, x) plt.xlabel(&quot;Time [second]&quot;); . . DFT produces N complex coefficients xk per each component with frequency of k. Here, k denotes normalized frequency with unit of cycle/point and it ranges from -0.5 to 0.5. We can convert these frequencies to Hertz, by substituting point with time interval between consecutive points. . $$ frac{cycle}{point} = frac{cycle}{ frac{t~seconds}{N~points}} = frac{N}{t} * frac{cycle}{second} = frac{N}{t}~Hz $$Numpy provides a utility function numpy.fft.fftfreq that takes number of points in DFT and timestep which is multiplicative inverse of sampling rate. . N = len(signal) xk = np.fft.fft(x) freq = np.fft.fftfreq(n=N, d=1/fs) . print(xk[:5]) . [12.07258851 +0.j 3.04180456+13.42630117j 2.4691799 -4.18227651j 1.64226413 -0.16802957j 5.91780044 +1.05461309j] . print(freq) . [ 0. 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53. 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. -64. -63. -62. -61. -60. -59. -58. -57. -56. -55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42. -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28. -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14. -13. -12. -11. -10. -9. -8. -7. -6. -5. -4. -3. -2. -1.] . # the frequencies normalized_freq = np.fft.fftfreq(n=N) freq_manual = normalized_freq * N / T assert np.allclose(freq, freq_manual) . # Bring negative frequencies to the front of the array fr = np.roll(freq, N//2) amp = np.roll(np.abs(xk), N//2) phase = np.roll(np.angle(xk), N//2) . #collapse-hide plt.figure(figsize=(12, 4)) plt.bar(fr, amp) plt.ylabel(&quot;Amplitude&quot;); plt.xlabel(&quot;Frequency [Hz]&quot;) . . Text(0.5, 0, &#39;Frequency [Hz]&#39;) . Inverse Discrete Fourier Transform . $ x_n = frac{1}{N} sum_{k=0}^{N-1} X_k e^{i~2 pi~k~n~/~N} $ . Now, we have the information about each of the components in the signal. We can reconstruct the original signal, by combining those components. For this, we first convert each coefficient xk into cosine and sine waves by multiplying with $e^{i~2 pi~k~n~/~N}$. Then, we sum all these waves per component and normalize it by number of points. . Numpy provides inverse FFT function numpy.fft.ifft, which takes N complex coefficients xk and outputs N complex numbers with very small imaginary parts. Remember, . $$e^{ix} = cos (x)+i sin (x)$$ $$ cos(-x) = cos(x) $$ $$ sin(-x) = -sin(x) $$ . Therefore, while summing up the components, the imaginary parts (sine) for negative and positive frequencies cancels each other. Whereas, real parts (cosine) add up. . rec_signal = np.real(np.fft.ifft(xk)) . #collapse-hide plt.figure(figsize=(12, 6)) plt.plot(signal, lw=2) plt.plot(rec_signal, lw=2) plt.xlabel(&quot;Time [second]&quot;) plt.legend([&quot;Original&quot;, &quot;Reconstructed&quot;]); . . Components . Let&#39;s see how FFT predicts the amplitude, frequency and phase parameters of the components. . #collapse-hide def get_component_params(xk, freq): component = 2 * xk[freq] # multiply by two to include -freq as well phase = np.angle(component) amplitude = np.abs(component) / N return amplitude, phase . . #collapse-hide def make_component_signal(xk, freq, t): amplitude, phase = get_component_params(xk, freq) component_signal = amplitude * np.cos(2 * np.pi * freq * t + phase) return component_signal . . #collapse-hide for a, f, ph in components: amp, phase = get_component_params(xk, f) print() print(f&quot;Component {f} Hz&quot;) print(&#39;_&#39;*40) print() print(f&quot;True amplitude: {a:4.2f} phase: {np.rad2deg(ph):4.1f} deg&quot;) print(f&quot;Calc. amplitude: {amp:4.2f} phase: {np.rad2deg(phase):4.1f} deg&quot;) print(&#39;=&#39;*40) . . Component 5 Hz ________________________________________ True amplitude: 1.00 phase: 45.0 deg Calc. amplitude: 1.07 phase: 52.1 deg ======================================== Component 10 Hz ________________________________________ True amplitude: 2.00 phase: 0.0 deg Calc. amplitude: 1.97 phase: 17.2 deg ======================================== . As we can see, it is fairly accurate considering the noise added on the signal. . #collapse-hide c5 = make_component_signal(xk, 5, t) c10 = make_component_signal(xk, 10, t) plt.figure(figsize=(12, 6)) plt.plot(t, signal, linewidth=2) plt.plot(t, c5, t, c10) plt.xlabel(&quot;Time [second]&quot;) plt.legend([&#39;Original signal&#39;, &#39;5 Hz component&#39;, &#39;10 Hz component&#39;]); . . Conclusion . FFT transforms a signal from time domain to frequency domain and for some problems, frequency domain is more feasible to work with. FFT has usages in many fields such as . Solving differential equations | Signal filtering algorithms | System identification | .",
            "url": "https://bdsaglam.github.io/blog/math/2020/06/23/fourier-transform.html",
            "relUrl": "/math/2020/06/23/fourier-transform.html",
            "date": " • Jun 23, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Barιş Deniz. I’m a self-taught software engineer with BSc. in Mechanical Engineering at Middle East Technical University. I’m passionate about understanding brain and artificial intelligence. I have hands-on experience in machine learning, computer vision, reinforcement learning and mobile development. . Posts in English and Turkish. . You can reach me out on Twitter, LinkedIn or Github. .",
          "url": "https://bdsaglam.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://bdsaglam.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}