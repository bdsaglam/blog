{
  
    
        "post0": {
            "title": "Why initialization is important in neural networks?",
            "content": "Today, when you build a deep neural network with standard layers and activation functions, you usually don&#39;t encounter much problems in training process. But, this wasn&#39;t always so. Exploding/vanishing gradients and numerical instability issues were common. We don&#39;t often have such problems anymore, mostly because ReLU has become the default non-linearity and modern neural network frameworks uses the right initialization schemes under the hood. But, why is initialization important? . We can consider a neural network as a collection learnable parameters that are initialized randomly and updated with gradient descent. In theory, this must find the optimal parameters, no matter the initial values. However, since the floating point representations used in computers have a finite range and precision, picking wrong initial values may cause out-of-range numerical values and cripple training process. . OK, how about constraining all the initial values to a specific range, such as [-1, 1] or [-0.1, 0.1]? Would it solve the problem? Unfortunately, it may not. In modern deep neural networks, the input goes through many layers, each changing incoming data in a specific way. Therefore, the changes accumulate through the network and eventually may cause the values grow too large or too small. To solve this problem for all networks with arbitrary number of layers, we use proven initialization schemes that keep the variance of data and gradients unchanged through neural network as much as possible. . How does the properties of a distribution changes through various transformations? . #collapse-hide %matplotlib inline import matplotlib.pyplot as plt import seaborn # Set seaborn aesthetic parameters to defaults seaborn.set() seaborn.set_style(&#39;whitegrid&#39;) seaborn.set_palette(&quot;dark&quot;) import numpy as np import torch np.random.seed(42) def plot_distributions(*arrays, bins=None, fig_width=12, fig_height=4, xlim=None): n = len(arrays) fig = plt.figure(figsize=(fig_width, fig_height*n)) for i, x in enumerate(arrays): ax = fig.add_subplot(n, 1, i+1) seaborn.distplot(x, bins=bins) ax.set_title(stats_desc(x)) ax.set_xlim(xlim) . . def plot_distribution(x, bins=None, fig_width=12, fig_height=4, xlim=None): fig = plt.figure(figsize=(fig_width, fig_height)) seaborn.distplot(x, bins=bins) plt.title(name + &#39; &#39; + stats_desc(x)) plt.xlim(xlim) . def stats_desc(x): return f&quot;μ: {x.mean():+.3f} σ: {x.std():.3f}&quot; . Let&#39;s assume that the input is from a uniform distribution between [-1, 1]. . $x sim U[-1, 1]$ . n = 1000 x = np.random.uniform(-1, 1, n) plot_distributions(x, bins=100) . Summation . The sum of two independent uniform distribution is a distribution with . $ quad mu= mu_1 + mu_2 quad sigma^2= sigma_1^2 + sigma_2^2$ . y = np.random.uniform(0, 10, n) z = x + y print(f&quot;Added a uniform dist. {stats_desc(y)}&quot;) plot_distributions(x, y, xlim=(-2, 15), fig_width=12) . Added a uniform dist. μ: +5.070 σ: 2.920 . Scale and shift . y = 2 * x + 1 plot_distributions(x, y, xlim=(-4, 4)) . Affine transformation . A linear layer (or fully connected layer) with weights, $W sim U[-c, c]$ , approximately transforms a uniform distribution $x sim U[-1, 1]$ to a normal distribution $y sim mathcal N( mu, sigma)$ . $$ mu = 0 quad sigma^2 = n_{in} ~ sigma_{input}^2 ~ sigma_{w}^2 $$ . $n_{in}$: size of input, also denoted as, $ text{fan}_{in}$ . Let&#39;s initialize weights with $W sim U[-1, 1]$. . W = np.random.uniform(-1, 1, size=(n, n)) bias = np.random.uniform(-1, 1, size=n) print(f&quot;Weight {stats_desc(W)}&quot;) print(f&quot;Bias {stats_desc(bias)}&quot;) y = x @ W + bias plot_distributions(x, y, xlim=(-30, 30)) . Weight μ: +0.001 σ: 0.577 Bias μ: +0.008 σ: 0.566 . The variance increased dramatically even after one layer. . Let&#39;s try smaller weights with $W sim U[-0.1, 0.1]$ . W = np.random.uniform(-.1, .1, size=(n, n)) bias = np.random.uniform(-.1, .1, size=n) print(f&quot;Weight {stats_desc(W)}&quot;) print(f&quot;Bias {stats_desc(bias)}&quot;) y = x @ W + bias plot_distributions(x, y, xlim=(-4, 4)) . Weight μ: -0.000 σ: 0.058 Bias μ: +0.002 σ: 0.057 . This looks better, the variance goes up slightly. Let&#39;s see how it performs with multiple layers with various sizes. . class Affine: def __init__(self, dim_in, dim_out, weight_bound=0.1, bias_bound=0.1): self.W = np.random.uniform(-weight_bound, weight_bound, size=(dim_in, dim_out)) self.b = np.random.uniform(-bias_bound, bias_bound, size=dim_out) def __call__(self, x): return x @ self.W + self.b . layers = [ Affine(x.shape[-1], 256), Affine(256, 512), Affine(512, 1024), Affine(1024, 1024), Affine(1024, 1024), Affine(1024, 1024), ] . activations = [] y = x for layer in layers: y = layer(y) activations.append(y) . plot_distributions(x, *activations, xlim=(-25, 25)) . The variance keeps increasing after each layer and at the end it is much larger than the beginning&#39;s. . Solution . We need to keep variance of data and gradients unchanged through the network as much as possible. . In forward pass, . $$ sigma^2 = n_{in} ~ sigma_{input}^2 ~ sigma_{w}^2 $$ . To keep variance unchanged, we want to have . $$ n_{in} ~ sigma_w^2 = 1 quad sigma_{w}^2 = frac{1}{n_{in}} $$ . A similar tranformation happens in backward pass when gradients flow in reverse direction. . $$ sigma_ text{grad before layer}^2 = n_{out}~ sigma_{w}^2~ sigma_{ text{grad after layer}}^2 $$ . Again, we want to have . $$ n_{out} ~ sigma_w^2 = 1 quad sigma_{w}^2 = frac{1}{n_{out}} $$ . Unless $n_{in}=n_{out}$, we cannot satisfy both constraints. Therefore, we do our best. . $$ frac{1}{2} (n_{in} + n_{out}) sigma_{w}^2 = 1 sigma_w^2 = frac{2}{n_{in} + n_{out}} $$Remember the variance of a uniform distribution with U[a, b] . $$ sigma^2 = frac{(b-a)^2}{12} $$ . $$ sigma_{w}^2 = frac{(2c)^2}{12} = frac{c^2}{3} $$ . Hence, . $$ frac{c^2}{3} = frac{2}{n_{in} + n_{out}} quad c = sqrt{ frac{6}{n_{in} + n_{out}}} $$ . This is known as Xavier initialization. . Now, let&#39;s initialize the weights according to Xavier initialization, see how it affects variance. . class XavierAffine: def __init__(self, dim_in, dim_out): weight_bound = np.sqrt(6/(dim_in + dim_out)) bias_bound = np.sqrt(2/(dim_in + dim_out)) self.W = np.random.uniform(-weight_bound, weight_bound, size=(dim_in, dim_out)) self.b = np.random.uniform(-bias_bound, bias_bound, size=dim_out) def __call__(self, x): return x @ self.W + self.b . xav_layers = [ XavierAffine(x.shape[-1], 256), XavierAffine(256, 512), XavierAffine(512, 1024), XavierAffine(1024, 1024), XavierAffine(1024, 1024), XavierAffine(1024, 1024), ] . activations = [] y = x for layer in xav_layers: y = layer(y) activations.append(y) . plot_distributions(x, *activations, xlim=(-2, 2)) . As it&#39;s seen, with Xaiver initialization, the variance does not vary much through the layers. . References . http://d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html#xavier-initialization |",
            "url": "https://bdsaglam.github.io/blog/neural-network/2020/06/25/initialization-in-nn.html",
            "relUrl": "/neural-network/2020/06/25/initialization-in-nn.html",
            "date": " • Jun 25, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fourier Transform",
            "content": "Fourier transform decomposes a signal into the sinusoidal components with different amplitude, frequency and phases such that the sum of those components equals to the signal. It transforms a signal from time domain to frequency domain. Fourier transform is applicable to both continuous and discrete signals. In this post, we will only cover the discrete case. . For a signal with N points, Discrete Fourier Transform uses the following bases: . $e^{i~2 pi~k} = cos (2 pi~k)+i sin (2 pi~k)$ . for k = -N/2 ... N/2 where k=1 means 1 cycle per signal. . Nyquist&#39;s Sampling Theorem . To be able to measure a sinusoidal wave, we need to sample at least two points within its one full cycle, i.e., one point in half cycle. Therefore, we cannot measure the components that make more than N/2 cycles within N timesteps. This is known as Nyquist&#39;s sampling theorem. In practice, because of the risk of sampling those two points near zero crossings, we are only confident about the components with frequencies lower than N/4. . As an example, let&#39;s see the measurements from 1 Hz sine wave with various sampling rates. Nyquist&#39;s sampling theorem requires at least 2 Hz sampling rate for this signal. As you can see in the figure below, the sampling rate of 1 Hz (blue) measures a flat line. While sampling rate of 2 Hz (orange) is theoretically sufficent, its amplitude is far from the original&#39;s. As the sampling rate increases, the measurement becomes more accurate. . %matplotlib inline import matplotlib.pyplot as plt import numpy as np . # Original signal w = 1 # frequency of sine wave, [Hz] T = 3 # duration of signal [second] fs = 128 # underlying sampling rate of signal[Hz] t = np.linspace(0, T, fs * T) y = np.sin(2*np.pi*w*t) indices = np.arange(len(t)) fs_list = [1, 2, 4, 10] samples = [] for f in fs_list: offset = np.random.randint(0, 10) idx = indices[:-offset:fs//f] + offset t_sampled = t[idx] y_sampled = y[idx] samples.append((f, t_sampled, y_sampled)) . #collapse-hide plt.figure(figsize=(12, 8)) plt.plot(t, y, color=&#39;magenta&#39;, lw=4); legends = [&#39;Original signal&#39;] for f, ts, ys in samples: plt.plot(ts, ys, marker=&#39;o&#39;, ms=10, linestyle=&#39;--&#39;, lw=4); legends.append(f&#39;Fsamp = {f}ω&#39;) plt.legend(legends, loc=&#39;lower left&#39;) plt.xlim([0, 2]); . . Discrete Fourier Transform . Discrete Fourier Transform . $ X_k = sum_{n=0}^{N-1} x_n cdot e^{-i~2 pi~k~n~/~N} $ . Inverse Discrete Fourier Transform . $ x_n = frac{1}{N} sum_{k=0}^{N-1} X_k e^{i~2 pi~k~n~/~N} $ . The intuition behind first formula is that kth cosine component takes k/N cycles in 1 timestep or point interval. In other words, its angular velocity is 2πk/N radians/timestep. To make kth component&#39;s peak align with nth point, we need adjust the phase of kth component. Since it takes 2πk/N radians in one timestep, until nth point, it takes 2πkn/N radians. Therefore, we need to delay this component for 2πkn/N radians. To delay, we need to subtract from its phase, which means rotating its complex representation in negative/counter-clock-wise direction. Hence, there is a minus sign in front. . From the above formula, for N point signal, a naive Fourier transform algorithm has O(N2) time complexity. However, Cooley and Tukey (1965) proposed an O(N logN) algorithm; hence, named as Fast Fourier Transform. It benefits the symmetry in the problem and uses recursive divide-conquer approach. Checkout this detailed explanation of FFT algorithm, if you&#39;re interested. . Numpy provides FFT algorithm in numpy.fft subpackage along with some utilities. . Let&#39;s have a signal consisting of two sinusoidal waves with 5 Hz and 10 Hz and uniform noise. Since FFT makes recursive calls to divide the signal into two halves, the number of points in the signal must be power of 2. . fs = 128 # sampling rate, [Hz] T = 1 # duration of signal, [second] t = np.linspace(0, T, fs) N = len(t) . components = [ (1, 5, np.pi/4), # amplitude, frequency [Hz], phase [radian] (2, 10, 0), ] signal = sum([a * np.cos(2*np.pi*w*t + p) for (a, w, p) in components]) noise = 1*np.random.uniform(-1, 1, N) x = signal + noise . #collapse-hide plt.figure(figsize=(12, 6)) plt.plot(t, x) plt.xlabel(&quot;Time [second]&quot;); . . DFT produces N complex coefficients xk per each component with frequency of k. Here, k denotes normalized frequency with unit of cycle/point and it ranges from -0.5 to 0.5. We can convert these frequencies to Hertz, by substituting point with time interval between consecutive points. . $$ frac{cycle}{point} = frac{cycle}{ frac{t~seconds}{N~points}} = frac{N}{t} * frac{cycle}{second} = frac{N}{t}~Hz $$Numpy provides a utility function numpy.fft.fftfreq that takes number of points in DFT and timestep which is multiplicative inverse of sampling rate. . N = len(signal) xk = np.fft.fft(x) freq = np.fft.fftfreq(n=N, d=1/fs) . print(xk[:5]) . [12.07258851 +0.j 3.04180456+13.42630117j 2.4691799 -4.18227651j 1.64226413 -0.16802957j 5.91780044 +1.05461309j] . print(freq) . [ 0. 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53. 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. -64. -63. -62. -61. -60. -59. -58. -57. -56. -55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42. -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28. -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14. -13. -12. -11. -10. -9. -8. -7. -6. -5. -4. -3. -2. -1.] . # the frequencies normalized_freq = np.fft.fftfreq(n=N) freq_manual = normalized_freq * N / T assert np.allclose(freq, freq_manual) . # Bring negative frequencies to the front of the array fr = np.roll(freq, N//2) amp = np.roll(np.abs(xk), N//2) phase = np.roll(np.angle(xk), N//2) . #collapse-hide plt.figure(figsize=(12, 4)) plt.bar(fr, amp) plt.ylabel(&quot;Amplitude&quot;); plt.xlabel(&quot;Frequency [Hz]&quot;) . . Text(0.5, 0, &#39;Frequency [Hz]&#39;) . Inverse Discrete Fourier Transform . $ x_n = frac{1}{N} sum_{k=0}^{N-1} X_k e^{i~2 pi~k~n~/~N} $ . Now, we have the information about each of the components in the signal. We can reconstruct the original signal, by combining those components. For this, we first convert each coefficient xk into cosine and sine waves by multiplying with $e^{i~2 pi~k~n~/~N}$. Then, we sum all these waves per component and normalize it by number of points. . Numpy provides inverse FFT function numpy.fft.ifft, which takes N complex coefficients xk and outputs N complex numbers with very small imaginary parts. Remember, . $$e^{ix} = cos (x)+i sin (x)$$ $$ cos(-x) = cos(x) $$ $$ sin(-x) = -sin(x) $$ . Therefore, while summing up the components, the imaginary parts (sine) for negative and positive frequencies cancels each other. Whereas, real parts (cosine) add up. . rec_signal = np.real(np.fft.ifft(xk)) . #collapse-hide plt.figure(figsize=(12, 6)) plt.plot(signal, lw=2) plt.plot(rec_signal, lw=2) plt.xlabel(&quot;Time [second]&quot;) plt.legend([&quot;Original&quot;, &quot;Reconstructed&quot;]); . . Components . Let&#39;s see how FFT predicts the amplitude, frequency and phase parameters of the components. . #collapse-hide def get_component_params(xk, freq): component = 2 * xk[freq] # multiply by two to include -freq as well phase = np.angle(component) amplitude = np.abs(component) / N return amplitude, phase . . #collapse-hide def make_component_signal(xk, freq, t): amplitude, phase = get_component_params(xk, freq) component_signal = amplitude * np.cos(2 * np.pi * freq * t + phase) return component_signal . . #collapse-hide for a, f, ph in components: amp, phase = get_component_params(xk, f) print() print(f&quot;Component {f} Hz&quot;) print(&#39;_&#39;*40) print() print(f&quot;True amplitude: {a:4.2f} phase: {np.rad2deg(ph):4.1f} deg&quot;) print(f&quot;Calc. amplitude: {amp:4.2f} phase: {np.rad2deg(phase):4.1f} deg&quot;) print(&#39;=&#39;*40) . . Component 5 Hz ________________________________________ True amplitude: 1.00 phase: 45.0 deg Calc. amplitude: 1.07 phase: 52.1 deg ======================================== Component 10 Hz ________________________________________ True amplitude: 2.00 phase: 0.0 deg Calc. amplitude: 1.97 phase: 17.2 deg ======================================== . As we can see, it is fairly accurate considering the noise added on the signal. . #collapse-hide c5 = make_component_signal(xk, 5, t) c10 = make_component_signal(xk, 10, t) plt.figure(figsize=(12, 6)) plt.plot(t, signal, linewidth=2) plt.plot(t, c5, t, c10) plt.xlabel(&quot;Time [second]&quot;) plt.legend([&#39;Original signal&#39;, &#39;5 Hz component&#39;, &#39;10 Hz component&#39;]); . . Conclusion . FFT transforms a signal from time domain to frequency domain and for some problems, frequency domain is more feasible to work with. FFT has usages in many fields such as . Solving differential equations | Signal filtering algorithms | System identification | .",
            "url": "https://bdsaglam.github.io/blog/math/2020/06/23/fourier-transform.html",
            "relUrl": "/math/2020/06/23/fourier-transform.html",
            "date": " • Jun 23, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Barιş Deniz. I’m a self-taught software engineer with BSc. in Mechanical Engineering at Middle East Technical University. I’m passionate about understanding brain and artificial intelligence. I have hands-on experience in machine learning, computer vision, reinforcement learning and mobile development. . Posts in English and Turkish. . You can reach me out on Twitter, LinkedIn or Github. .",
          "url": "https://bdsaglam.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://bdsaglam.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}